{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\allis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import krippendorff\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from scipy.stats import chi2_contingency, ttest_1samp\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "main_dir = os.path.join(current_dir, '..')\n",
    "sys.path.append(main_dir)\n",
    "\n",
    "from humanise import humanise_sentence, initialise_globals\n",
    "\n",
    "initialise_globals(main_dir)\n",
    "HUMAN_DATAPATH = os.path.join(current_dir, 'human.txt')\n",
    "GENERATE_DATAPATH = os.path.join(current_dir, 'synthetic_generate_v3.txt')\n",
    "HUMANISE_DATAPATH = os.path.join(current_dir, 'synthetic_humanise_v3.txt')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Turing Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load human data from MaintNorm dataset\n",
    "def load_maintnorm_sentences(file_path):\n",
    "    dirty_sentences = []\n",
    "    clean_sentences = []\n",
    "    current_dirty = []\n",
    "    current_clean = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:  # Empty line means a new sentence\n",
    "                if current_dirty and current_clean:\n",
    "                    dirty_sentences.append(' '.join(current_dirty))\n",
    "                    clean_sentences.append(' '.join(current_clean))\n",
    "                    current_dirty = []\n",
    "                    current_clean = []\n",
    "            else:\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) > 1:\n",
    "                    dirty, clean = parts[0], parts[1]\n",
    "                    if not clean in ['<id>', '-']:\n",
    "                        current_dirty.append(dirty.lower())\n",
    "                    current_clean.append(clean.lower())\n",
    "    return dirty_sentences, clean_sentences\n",
    "\n",
    "# Save all human data to text file\n",
    "def save_human_data():\n",
    "    train_dirty, _ = load_maintnorm_sentences('../data/MaintNorm/train.norm')\n",
    "    test_dirty, _ = load_maintnorm_sentences('../data/MaintNorm/test.norm')\n",
    "    val_dirty, _ = load_maintnorm_sentences('../data/MaintNorm/val.norm')\n",
    "    full_dirty = train_dirty + test_dirty + val_dirty\n",
    "    human_data = list(set(full_dirty)) # remove duplicates\n",
    "    with open(HUMAN_DATAPATH, 'w') as f:\n",
    "        for item in human_data:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "# Function to load human sentences or synthetic sentences\n",
    "def load_sentences(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [line.strip() for line in file]\n",
    "\n",
    "# Function to sample by sections\n",
    "def synthetic_sample(data, num_samples, num_sections):\n",
    "    section_size = len(data) // num_sections\n",
    "    syn_sections = [data[i*section_size:(i+1)*section_size] for i in range(num_sections)]\n",
    "    i = 0\n",
    "    for d in data[num_sections*section_size:]:\n",
    "        syn_sections[i].append(d)\n",
    "        i += 1\n",
    "    sample_size = math.ceil(num_samples / num_sections)\n",
    "    syn_samples = []\n",
    "    for section in syn_sections:\n",
    "        syn_samples.extend(random.sample(section, sample_size))\n",
    "    syn_samples = random.sample(syn_samples, 50)\n",
    "    return syn_samples\n",
    "\n",
    "# Uncomment to save human data to text file\n",
    "# save_human_data()\n",
    "\n",
    "# Humanise generated synthetic data\n",
    "synthetic_data = load_sentences(GENERATE_DATAPATH)\n",
    "humanise_data = [humanise_sentence(s) for s in synthetic_data]\n",
    "\n",
    "with open(HUMANISE_DATAPATH, 'w') as f:\n",
    "    for item in humanise_data:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random 50 human data sentences\n",
    "human_data = load_sentences(HUMAN_DATAPATH)\n",
    "human_50 = random.sample(human_data, 50)\n",
    "human_50 = pd.DataFrame(human_50, columns=['sentence'])\n",
    "human_50['label'] = 'h'\n",
    "\n",
    "# Random 50 synthetic data sentences\n",
    "synthetic_data = load_sentences(HUMANISE_DATAPATH)\n",
    "synthetic_50 = synthetic_sample(synthetic_data, 50, 8)\n",
    "synthetic_50 = pd.DataFrame(synthetic_50, columns=['sentence'])\n",
    "synthetic_50['label'] = 's'\n",
    "\n",
    "# Combine and shuffle human and synthetic data\n",
    "turing_data = pd.concat([human_50, synthetic_50])\n",
    "turing_data = turing_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "turing_data.to_csv('target.csv', index=False)\n",
    "turing_data.drop(columns=['label']).to_csv('turing.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate annotators on Turing Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_turing(target_file, turing_file, print_results=True):\n",
    "    # Read files\n",
    "    target = pd.read_csv(target_file)\n",
    "    turing = pd.read_csv(turing_file)\n",
    "    turing['label'] = turing['label'].str.lower()\n",
    "    combine = pd.merge(target, turing, on='sentence')\n",
    "\n",
    "    # Counts\n",
    "    tp = ((combine['label_x'] == 'h') & (combine['label_y'] == 'h')).sum()\n",
    "    tn = ((combine['label_x'] == 's') & (combine['label_y'] == 's')).sum()\n",
    "    fp = ((combine['label_x'] == 's') & (combine['label_y'] == 'h')).sum()\n",
    "    fn = ((combine['label_x'] == 'h') & (combine['label_y'] == 's')).sum()\n",
    "\n",
    "    # Chi-square test\n",
    "    contingency_table = [[tp, fp], [fn, tn]]\n",
    "    res = chi2_contingency(contingency_table)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    column_names = ['Actual Human', 'Actual Synthetic']\n",
    "    index_names = ['Predicted Human', 'Predicted Synthetic']\n",
    "    confusion_matrix = pd.DataFrame(contingency_table, columns=column_names, index=index_names)\n",
    "\n",
    "    # Accuracy, Precision, Recall, F1-score\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # Percentage of human and synthetic data\n",
    "    num_human = turing['label'].value_counts()['h']\n",
    "    num_synthetic = turing['label'].value_counts()['s']\n",
    "    human_percentage = num_human / len(turing)\n",
    "    synthetic_percentage = num_synthetic / len(turing)\n",
    "\n",
    "    # Print results\n",
    "    if print_results:\n",
    "        print('-------------------------------- Frequency of Labels')\n",
    "        print(f'Labelled human      : {num_human} ({human_percentage:.2f})')\n",
    "        print(f'Labelled synthetic  : {num_synthetic} ({synthetic_percentage:.2f})')\n",
    "        \n",
    "        print('---------------------------------- Confusion Matrix')\n",
    "        print(confusion_matrix)\n",
    "        \n",
    "        print('----------------------------------- Chi-Square Test')\n",
    "        print(f'Chi-square          : {res.statistic:.4f}')\n",
    "        print(f'p-value             : {res.pvalue:.4f}')\n",
    "        print(f'Degrees of freedom  : {res.dof}')\n",
    "        print('Expected frequencies:')\n",
    "        print(res.expected_freq)\n",
    "\n",
    "        print('--------------------------------------- Performance')\n",
    "        print(f'Accuracy            : {accuracy:.2f}')\n",
    "        print(f'Precision           : {precision:.2f}')\n",
    "        print(f'Recall              : {recall:.2f}')\n",
    "        print(f'F1-score            : {f1_score:.2f}')\n",
    "    \n",
    "    return (accuracy, precision, recall, f1_score, human_percentage)\n",
    "\n",
    "def filter_predictions(target_file, turing_file):\n",
    "    target = pd.read_csv(target_file)\n",
    "    turing = pd.read_csv(turing_file) \n",
    "    turing['label'] = turing['label'].str.lower()\n",
    "    combine = pd.merge(target, turing, on='sentence')\n",
    "\n",
    "    # Actually human, predicted synthetic\n",
    "    false_synthetic = combine[(combine['label_x'] == 'h') & (combine['label_y'] == 's')]['sentence']\n",
    "    # Actually synthetic, predicted human\n",
    "    false_human = combine[(combine['label_x'] == 's') & (combine['label_y'] == 'h')]['sentence']\n",
    "    \n",
    "    print('----------------------------------- False Synthetic')\n",
    "    for s in false_synthetic:\n",
    "        print(s)\n",
    "    print('--------------------------------------- False Human')\n",
    "    for s in false_human:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agreement evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_performance(turing_files, accuracies, precisions, recalls, f1_scores, human_percentages):\n",
    "    # Calculate overall average performance\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "    avg_human_percentage = sum(human_percentages) / len(human_percentages)\n",
    "    alpha_agreement = annotator_agreement(turing_files)\n",
    "    \n",
    "    # Print results\n",
    "    print('----------------------------------- Overall Results')\n",
    "    print(f'Average Accuracy    : {avg_accuracy:.2f}')\n",
    "    print(f'Average Precision   : {avg_precision:.2f}')\n",
    "    print(f'Average Recall      : {avg_recall:.2f}')\n",
    "    print(f'Average F1-score    : {avg_f1_score:.2f}')\n",
    "    print(f'Average Human %     : {avg_human_percentage:.2f}')\n",
    "    print(f'Annotator Agreement : {alpha_agreement:.2f}')\n",
    "    return (avg_accuracy, avg_precision, avg_recall, avg_f1_score, avg_human_percentage, alpha_agreement)\n",
    "\n",
    "def find_common(target_file, turing_files):\n",
    "    # Regex pattern extract name from filename\n",
    "    pattern = r'(?<=_)[^_]+(?=_v\\d+\\.csv)'\n",
    "    df = pd.read_csv(target_file)\n",
    "    for file in turing_files:\n",
    "        name = re.search(pattern, file).group()\n",
    "        pred = pd.read_csv(file)['label']\n",
    "        df[name] = pred\n",
    "    \n",
    "    # Find when everyone predicted human and actually synthetic\n",
    "    common_fp = df[(df['label'] == 's') & (df.iloc[:, 2:].eq('h').all(axis=1))].index\n",
    "    # Find when everyone predicted synthetic and actually human\n",
    "    common_fn = df[(df['label'] == 'h') & (df.iloc[:, 2:].eq('s').all(axis=1))].index\n",
    "    # Find when everyone predicted human and actually human\n",
    "    common_tp = df[(df['label'] == 'h') & (df.iloc[:, 2:].eq('h').all(axis=1))].index\n",
    "    # Find when everyone predicted synthetic and actually synthetic\n",
    "    common_tn = df[(df['label'] == 's') & (df.iloc[:, 2:].eq('s').all(axis=1))].index\n",
    "    \n",
    "    print('--------------------- Common Predicted Human Actually Synthetic')\n",
    "    for i in common_fp:\n",
    "        print(df.iloc[i, 0])\n",
    "    print('--------------------- Common Predicted Synthetic Actually Human')\n",
    "    for i in common_fn:\n",
    "        print(df.iloc[i, 0])\n",
    "    print('----------------- Common Predicted Synthetic Actually Synthetic')\n",
    "    for i in common_tn:\n",
    "        print(df.iloc[i, 0])\n",
    "    print('------------------------- Common Predicted Human Actually Human')\n",
    "    for i in common_tp:\n",
    "        print(df.iloc[i, 0])\n",
    "\n",
    "# Krippendorff's Alpha for annotator agreement\n",
    "def annotator_agreement(turing_files):\n",
    "    labels = []\n",
    "    for file in turing_files:\n",
    "        label = pd.read_csv(file)['label'].tolist()\n",
    "        labels.append(label)\n",
    "    alpha = krippendorff.alpha(reliability_data=labels, level_of_measurement='nominal')\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate individual predictions V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "label\n",
      "h    54\n",
      "s    45\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                29                25\n",
      "Predicted Synthetic            20                25\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 0.5122\n",
      "p-value             : 0.4742\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[26.72727273 27.27272727]\n",
      " [22.27272727 22.72727273]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.5455\n"
     ]
    }
   ],
   "source": [
    "evaluate_turing('Version1/target_v1.csv', 'Version1/turing_jf_v1.csv')\n",
    "# filter_predictions('Version1/target_v1.csv', 'Version1/turing_jf_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 56 (0.56)\n",
      "Labelled synthetic  : 44 (0.44)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                47                 9\n",
      "Predicted Synthetic             3                41\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 55.5601\n",
      "p-value             : 0.0000\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[28. 28.]\n",
      " [22. 22.]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.88\n",
      "Precision           : 0.84\n",
      "Recall              : 0.94\n",
      "F1-score            : 0.89\n"
     ]
    }
   ],
   "source": [
    "evaluate_turing('Version1/target_v1.csv', 'Version1/turing_cg_v1.csv')\n",
    "# filter_predictions('Version1/target_v1.csv', 'Version1/turing_cg_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 65 (0.65)\n",
      "Labelled synthetic  : 35 (0.35)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                40                25\n",
      "Predicted Synthetic            10                25\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 8.6154\n",
      "p-value             : 0.0033\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[32.5 32.5]\n",
      " [17.5 17.5]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.65\n",
      "Precision           : 0.62\n",
      "Recall              : 0.80\n",
      "F1-score            : 0.70\n"
     ]
    }
   ],
   "source": [
    "evaluate_turing('Version1/target_v1.csv', 'Version1/turing_ms_v1.csv')\n",
    "# filter_predictions('Version1/target_v1.csv', 'Version1/turing_ms_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 40 (0.40)\n",
      "Labelled synthetic  : 60 (0.60)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                32                 8\n",
      "Predicted Synthetic            18                42\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 22.0417\n",
      "p-value             : 0.0000\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[20. 20.]\n",
      " [30. 30.]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.74\n",
      "Precision           : 0.80\n",
      "Recall              : 0.64\n",
      "F1-score            : 0.71\n"
     ]
    }
   ],
   "source": [
    "evaluate_turing('Version1/target_v1.csv', 'Version1/turing_mh_v1.csv')\n",
    "# filter_predictions('Version1/target_v1.csv', 'Version1/turing_mh_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 52 (0.52)\n",
      "Labelled synthetic  : 48 (0.48)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                27                25\n",
      "Predicted Synthetic            23                25\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 0.0401\n",
      "p-value             : 0.8414\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[26. 26.]\n",
      " [24. 24.]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.52\n",
      "Precision           : 0.52\n",
      "Recall              : 0.54\n",
      "F1-score            : 0.53\n"
     ]
    }
   ],
   "source": [
    "evaluate_turing('Version1/target_v1.csv', 'Version1/turing_cw_v1.csv')\n",
    "# filter_predictions('Version1/target_v1.csv', 'Version1/turing_cw_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate annotator agreement V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------- Overall Results\n",
      "Average Accuracy    : 0.64\n",
      "Average Precision   : 0.64\n",
      "Average Recall      : 0.66\n",
      "Average F1-score    : 0.65\n",
      "Average Human %     : 0.52\n",
      "Annotator Agreement : 0.17\n"
     ]
    }
   ],
   "source": [
    "evaluators = [\n",
    "    'Version1/turing_ms_v1.csv',\n",
    "    'Version1/turing_mh_v1.csv',\n",
    "    'Version1/turing_cw_v1.csv'\n",
    "]\n",
    "\n",
    "accuracy, precision, recall, f1_score, human_percentage = [], [], [], [], []\n",
    "for e in evaluators:\n",
    "    results = evaluate_turing('Version1/target_v1.csv', e, False)\n",
    "    accuracy.append(results[0])\n",
    "    precision.append(results[1])\n",
    "    recall.append(results[2])\n",
    "    f1_score.append(results[3])\n",
    "    human_percentage.append(results[4])\n",
    "\n",
    "overall_results = average_performance(evaluators, accuracy, precision, \n",
    "                                      recall, f1_score, human_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Common Predicted Human Actually Synthetic\n",
      "replace leaking lube pump\n",
      "hmu leaking hydraulic fluid\n",
      "decking has several cracks\n",
      "boom foot clevbis pin has no grease\n",
      "diff lube hose insp for leaks\n",
      "--------------------- Common Predicted Synthetic Actually Human\n",
      "replace pos 8 wheel end po\n",
      "replace faulty brake sensor1 task\n",
      "cw coolant leak from #15 cylind\n",
      "pcr room over alarm\n",
      "----------------- Common Predicted Synthetic Actually Synthetic\n",
      "chge out leaking axle oil cool\n",
      "leak detected infan pump\n",
      "cabindoor is leaking\n",
      "leak in air aircon hose\n",
      "replace leaking air aircon hose\n",
      "plug has a leak\n",
      "leaikng fluid fr swingbrake pump\n",
      "gasket isleaking\n",
      "hea t ssink clamp found brnk\n",
      "machine isn't starting\n",
      "hst motor hose shows a leak\n",
      "cabin roof heater taps show leakking\n",
      "engine fan pmp hose has a leak\n",
      "------------------------- Common Predicted Human Actually Human\n",
      "change out tyre pos 0\n",
      "replace leaky lh tilt cyl hose\n",
      "change out l/h lift cyl\n",
      "l/h side stick cylinder leak\n",
      "aftercooler gauge faulty\n",
      "mv3594- c/out steering ball studs\n",
      "replace bonnet hinges\n",
      "replace blown dash globes\n",
      "drag gen field exciter trips\n",
      "fw9593-fire suppression inspection\n",
      "xw7976- cw coolant leak from radiator\n",
      "major cable reroute\n",
      "2000 hr service\n",
      "repair blade tilt fault\n",
      "low transmission oil level\n",
      "repl diff cool hose to front diff\n"
     ]
    }
   ],
   "source": [
    "find_common('Version1/target_v1.csv', evaluators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate individual predictions V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 48 (0.48)\n",
      "Labelled synthetic  : 52 (0.52)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                31                17\n",
      "Predicted Synthetic            19                33\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 6.7708\n",
      "p-value             : 0.0093\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[24. 24.]\n",
      " [26. 26.]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.64\n",
      "Precision           : 0.65\n",
      "Recall              : 0.62\n",
      "F1-score            : 0.63\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.64, 0.6458333333333334, 0.62, 0.6326530612244898, 0.48)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_turing('Version2/target.csv', 'Version2/turing_cg.csv')\n",
    "# filter_predictions('Version2/target.csv', 'Version2/turing_cg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 49 (0.49)\n",
      "Labelled synthetic  : 51 (0.51)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                25                24\n",
      "Predicted Synthetic            25                26\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 0.0000\n",
      "p-value             : 1.0000\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[24.5 24.5]\n",
      " [25.5 25.5]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.51\n",
      "Precision           : 0.51\n",
      "Recall              : 0.50\n",
      "F1-score            : 0.51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.51, 0.5102040816326531, 0.5, 0.5050505050505051, 0.49)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_turing('Version2/target.csv', 'Version2/turing_jf.csv')\n",
    "# filter_predictions('Version2/target.csv', 'Version2/turing_jf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 63 (0.63)\n",
      "Labelled synthetic  : 37 (0.37)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                37                26\n",
      "Predicted Synthetic            13                24\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 4.2900\n",
      "p-value             : 0.0383\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[31.5 31.5]\n",
      " [18.5 18.5]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.61\n",
      "Precision           : 0.59\n",
      "Recall              : 0.74\n",
      "F1-score            : 0.65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.61, 0.5873015873015873, 0.74, 0.6548672566371682, 0.63)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_turing('Version2/target.csv', 'Version2/turing_ms.csv')\n",
    "# filter_predictions('Version2/target.csv', 'Version2/turing_ms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 64 (0.64)\n",
      "Labelled synthetic  : 36 (0.36)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                36                28\n",
      "Predicted Synthetic            14                22\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 2.1267\n",
      "p-value             : 0.1447\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[32. 32.]\n",
      " [18. 18.]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.58\n",
      "Precision           : 0.56\n",
      "Recall              : 0.72\n",
      "F1-score            : 0.63\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.58, 0.5625, 0.72, 0.631578947368421, 0.64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_turing('Version2/target.csv', 'Version2/turing_mh.csv')\n",
    "# filter_predictions('Version2/target.csv', 'Version2/turing_mh.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate annotator agreement V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------- Overall Results\n",
      "Average Accuracy    : 0.59\n",
      "Average Precision   : 0.57\n",
      "Average Recall      : 0.73\n",
      "Average F1-score    : 0.64\n",
      "Average Human %     : 0.64\n",
      "Annotator Agreement : 0.29\n"
     ]
    }
   ],
   "source": [
    "evaluators = [\n",
    "    'Version2/turing_ms.csv',\n",
    "    'Version2/turing_mh.csv'\n",
    "]\n",
    "\n",
    "accuracy, precision, recall, f1_score, human_percentage = [], [], [], [], []\n",
    "for e in evaluators:\n",
    "    results = evaluate_turing('Version2/target.csv', e, False)\n",
    "    accuracy.append(results[0])\n",
    "    precision.append(results[1])\n",
    "    recall.append(results[2])\n",
    "    f1_score.append(results[3])\n",
    "    human_percentage.append(results[4])\n",
    "\n",
    "overall_results = average_performance(evaluators, accuracy, precision, \n",
    "                                      recall, f1_score, human_percentage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
