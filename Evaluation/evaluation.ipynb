{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turing Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\allis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import krippendorff\n",
    "import statsmodels.api as sm\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import clear_output\n",
    "from scipy.stats import chi2_contingency\n",
    "from statsmodels.stats.inter_rater import aggregate_raters\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "main_dir = os.path.join(current_dir, '..')\n",
    "sys.path.append(main_dir)\n",
    "\n",
    "from Humanise.humanise import humanise_sentence, initialise_globals\n",
    "\n",
    "initialise_globals(main_dir)\n",
    "HUMAN_DATAPATH = os.path.join(current_dir, 'human.txt')\n",
    "GENERATE_DATAPATH = os.path.join(current_dir, 'Turing2/synthetic_generate_v2.txt')\n",
    "HUMANISE_DATAPATH = os.path.join(current_dir, 'Turing2/synthetic_humanise_v2.txt')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Turing Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load human data from MaintNorm dataset\n",
    "def load_maintnorm_sentences(file_path):\n",
    "    dirty_sentences = []\n",
    "    clean_sentences = []\n",
    "    current_dirty = []\n",
    "    current_clean = []\n",
    "    pattern = r'[A-Za-z]{2}\\d{4}-'\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:  # Empty line means a new sentence\n",
    "                if current_dirty and current_clean:\n",
    "                    dirty_sentences.append(' '.join(current_dirty))\n",
    "                    clean_sentences.append(' '.join(current_clean))\n",
    "                    current_dirty = []\n",
    "                    current_clean = []\n",
    "            else:\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) > 1:\n",
    "                    dirty, clean = parts[0], parts[1]\n",
    "                    if not clean in ['<id>', '-']:\n",
    "                        dirty = re.sub(pattern, '', dirty)\n",
    "                        current_dirty.append(dirty.lower().strip())\n",
    "                    current_clean.append(clean.lower())\n",
    "    return dirty_sentences, clean_sentences\n",
    "\n",
    "# Save all human data to text file\n",
    "def save_human_data():\n",
    "    train_dirty, _ = load_maintnorm_sentences('../data/MaintNorm/train.norm')\n",
    "    test_dirty, _ = load_maintnorm_sentences('../data/MaintNorm/test.norm')\n",
    "    val_dirty, _ = load_maintnorm_sentences('../data/MaintNorm/val.norm')\n",
    "    full_dirty = train_dirty + test_dirty + val_dirty\n",
    "    human_data = list(set(full_dirty)) # remove duplicates\n",
    "    with open(HUMAN_DATAPATH, 'w') as f:\n",
    "        for item in human_data:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "# Function to load human sentences or synthetic sentences\n",
    "def load_sentences(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [line.strip() for line in file]\n",
    "\n",
    "# Function to sample by sections\n",
    "def synthetic_sample(data, num_samples, num_sections):\n",
    "    section_size = len(data) // num_sections\n",
    "    syn_sections = [data[i*section_size:(i+1)*section_size] for i in range(num_sections)]\n",
    "    i = 0\n",
    "    for d in data[num_sections*section_size:]:\n",
    "        syn_sections[i].append(d)\n",
    "        i += 1\n",
    "    sample_size = math.ceil(num_samples / num_sections)\n",
    "    syn_samples = []\n",
    "    for section in syn_sections:\n",
    "        syn_samples.extend(random.sample(section, sample_size))\n",
    "    syn_samples = random.sample(syn_samples, 50)\n",
    "    return syn_samples\n",
    "\n",
    "# Function to generate random labels to test performance\n",
    "def random_labels(turing):\n",
    "    turing = pd.read_csv(turing)\n",
    "    turing['label'] = np.random.choice(['h', 's'], turing.shape[0])\n",
    "    turing.to_csv('Turing2/turing_random.csv', index=False)\n",
    "\n",
    "# Uncomment to save human data to text file\n",
    "save_human_data()\n",
    "\n",
    "# Humanise generated synthetic data\n",
    "# synthetic_data = load_sentences(GENERATE_DATAPATH)\n",
    "# humanise_data = [humanise_sentence(s) for s in synthetic_data]\n",
    "\n",
    "# with open(HUMANISE_DATAPATH, 'w') as f:\n",
    "#     for item in humanise_data:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random 50 human data sentences\n",
    "human_data = load_sentences(HUMAN_DATAPATH)\n",
    "human_50 = random.sample(human_data, 50)\n",
    "human_50 = pd.DataFrame(human_50, columns=['sentence'])\n",
    "human_50['label'] = 'h'\n",
    "\n",
    "# Random 50 synthetic data sentences\n",
    "synthetic_data = load_sentences(HUMANISE_DATAPATH)\n",
    "synthetic_50 = synthetic_sample(synthetic_data, 50, 8)\n",
    "synthetic_50 = pd.DataFrame(synthetic_50, columns=['sentence'])\n",
    "synthetic_50['label'] = 's'\n",
    "\n",
    "# Combine and shuffle human and synthetic data\n",
    "turing_data = pd.concat([human_50, synthetic_50])\n",
    "turing_data = turing_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "turing_data.to_csv('target.csv', index=False)\n",
    "turing_data.drop(columns=['label']).to_csv('turing.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate annotators on Turing Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_turing(target_file, turing_file, print_results=True):\n",
    "    # Read files\n",
    "    target = pd.read_csv(target_file)\n",
    "    turing = pd.read_csv(turing_file)\n",
    "    turing['label'] = turing['label'].str.lower()\n",
    "    combine = pd.merge(target, turing, on='sentence')\n",
    "\n",
    "    # Counts\n",
    "    tp = ((combine['label_x'] == 'h') & (combine['label_y'] == 'h')).sum()\n",
    "    tn = ((combine['label_x'] == 's') & (combine['label_y'] == 's')).sum()\n",
    "    fp = ((combine['label_x'] == 's') & (combine['label_y'] == 'h')).sum()\n",
    "    fn = ((combine['label_x'] == 'h') & (combine['label_y'] == 's')).sum()\n",
    "\n",
    "    # Chi-square test\n",
    "    contingency_table = [[tp, fp], [fn, tn]]\n",
    "    res = chi2_contingency(contingency_table)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    column_names = ['Actual Human', 'Actual Synthetic']\n",
    "    index_names = ['Predicted Human', 'Predicted Synthetic']\n",
    "    confusion_matrix = pd.DataFrame(contingency_table, columns=column_names, index=index_names)\n",
    "\n",
    "    # Accuracy, Precision, Recall, F1-score\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # Percentage of human and synthetic data\n",
    "    num_human = turing['label'].value_counts()['h']\n",
    "    num_synthetic = turing['label'].value_counts()['s']\n",
    "    human_percentage = num_human / len(turing)\n",
    "    synthetic_percentage = num_synthetic / len(turing)\n",
    "\n",
    "    # Print results\n",
    "    if print_results:\n",
    "        print('-------------------------------- Frequency of Labels')\n",
    "        print(f'Labelled human      : {num_human} ({human_percentage:.2f})')\n",
    "        print(f'Labelled synthetic  : {num_synthetic} ({synthetic_percentage:.2f})')\n",
    "        \n",
    "        print('---------------------------------- Confusion Matrix')\n",
    "        print(confusion_matrix)\n",
    "        \n",
    "        print('----------------------------------- Chi-Square Test')\n",
    "        print(f'Chi-square          : {res.statistic:.4f}')\n",
    "        print(f'p-value             : {res.pvalue:.4f}')\n",
    "        print(f'Degrees of freedom  : {res.dof}')\n",
    "        print('Expected frequencies:')\n",
    "        print(res.expected_freq)\n",
    "\n",
    "        print('--------------------------------------- Performance')\n",
    "        print(f'Accuracy            : {accuracy:.3f}')\n",
    "        print(f'Precision           : {precision:.3f}')\n",
    "        print(f'Recall              : {recall:.3f}')\n",
    "        print(f'F1-score            : {f1_score:.3f}')\n",
    "    \n",
    "    return (accuracy, precision, recall, f1_score, human_percentage)\n",
    "\n",
    "def filter_predictions(target_file, turing_file):\n",
    "    target = pd.read_csv(target_file)\n",
    "    turing = pd.read_csv(turing_file) \n",
    "    turing['label'] = turing['label'].str.lower()\n",
    "    combine = pd.merge(target, turing, on='sentence')\n",
    "\n",
    "    # Actually human, predicted synthetic\n",
    "    false_synthetic = combine[(combine['label_x'] == 'h') & (combine['label_y'] == 's')]['sentence']\n",
    "    # Actually synthetic, predicted human\n",
    "    false_human = combine[(combine['label_x'] == 's') & (combine['label_y'] == 'h')]['sentence']\n",
    "    \n",
    "    print('----------------------------------- False Synthetic')\n",
    "    for s in false_synthetic:\n",
    "        print(s)\n",
    "    print('--------------------------------------- False Human')\n",
    "    for s in false_human:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agreement evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_performance(turing_files, accuracies, precisions, recalls, f1_scores, human_percentages):\n",
    "    # Calculate overall average performance\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "    avg_human_percentage = sum(human_percentages) / len(human_percentages)\n",
    "    alpha_agreement = annotator_agreement(turing_files)\n",
    "    kappa_agreement = fleiss_kappa(turing_files)\n",
    "    \n",
    "    # Print results\n",
    "    print('----------------------------------- Overall Results')\n",
    "    print(f'Average Accuracy    : {avg_accuracy:.3f}')\n",
    "    print(f'Average Precision   : {avg_precision:.3f}')\n",
    "    print(f'Average Recall      : {avg_recall:.3f}')\n",
    "    print(f'Average F1-score    : {avg_f1_score:.3f}')\n",
    "    print(f'Average Human %     : {avg_human_percentage:.3f}')\n",
    "    print(f'Krippendorff Alpha  : {alpha_agreement:.3f}')\n",
    "    print(f'Fleiss Kappa        : {kappa_agreement:.3f}')\n",
    "    return (avg_accuracy, avg_precision, avg_recall, avg_f1_score, avg_human_percentage, alpha_agreement)\n",
    "\n",
    "def find_common(target_file, turing_files):\n",
    "    # Regex pattern extract name from filename\n",
    "    pattern = r'(?<=_)[^_]+(?=_v\\d+\\.csv)'\n",
    "    df = pd.read_csv(target_file)\n",
    "    for file in turing_files:\n",
    "        name = re.search(pattern, file).group()\n",
    "        pred = pd.read_csv(file)['label']\n",
    "        df[name] = pred\n",
    "    \n",
    "    # Find when everyone predicted human and actually synthetic\n",
    "    common_fp = df[(df['label'] == 's') & (df.iloc[:, 2:].eq('h').all(axis=1))].index\n",
    "    # Find when everyone predicted synthetic and actually human\n",
    "    common_fn = df[(df['label'] == 'h') & (df.iloc[:, 2:].eq('s').all(axis=1))].index\n",
    "    # Find when everyone predicted human and actually human\n",
    "    common_tp = df[(df['label'] == 'h') & (df.iloc[:, 2:].eq('h').all(axis=1))].index\n",
    "    # Find when everyone predicted synthetic and actually synthetic\n",
    "    common_tn = df[(df['label'] == 's') & (df.iloc[:, 2:].eq('s').all(axis=1))].index\n",
    "    \n",
    "    print(f'--------------------- Common Predicted Human Actually Synthetic ({len(common_fp)})')\n",
    "    for i in common_fp:\n",
    "        print(df.iloc[i, 0])\n",
    "    print(f'--------------------- Common Predicted Synthetic Actually Human ({len(common_fn)})')\n",
    "    for i in common_fn:\n",
    "        print(df.iloc[i, 0])\n",
    "    print(f'----------------- Common Predicted Synthetic Actually Synthetic ({len(common_tn)})')\n",
    "    for i in common_tn:\n",
    "        print(df.iloc[i, 0])\n",
    "    print(f'------------------------- Common Predicted Human Actually Human ({len(common_tp)})')\n",
    "    for i in common_tp:\n",
    "        print(df.iloc[i, 0])\n",
    "    \n",
    "    total_common = len(common_fp) + len(common_fn) + len(common_tp) + len(common_tn)\n",
    "    print('\\nTotal commonly labelled:', total_common)\n",
    "\n",
    "# Krippendorff's Alpha for annotator agreement\n",
    "def annotator_agreement(turing_files):\n",
    "    labels = [pd.read_csv(file)['label'].tolist() for file in turing_files]\n",
    "    alpha = krippendorff.alpha(reliability_data=labels, level_of_measurement='nominal')\n",
    "    return alpha\n",
    "\n",
    "# Fleiss' Kappa for annotator agreement\n",
    "def fleiss_kappa(turing_files):\n",
    "    labels = [pd.read_csv(file)['label'].tolist() for file in turing_files]\n",
    "    labels = np.array(pd.DataFrame(labels).T)\n",
    "    category_map = {'h': 1, 's': 0}\n",
    "    numeric_data = np.array([[category_map[label] for label in item] for item in labels])\n",
    "    contingency_table, _ = aggregate_raters(numeric_data, n_cat=2)\n",
    "    kappa = sm.stats.fleiss_kappa(contingency_table)\n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turing Test 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate individual predictions V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 54 (0.54)\n",
      "Labelled synthetic  : 45 (0.45)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                29                25\n",
      "Predicted Synthetic            20                25\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 0.5122\n",
      "p-value             : 0.4742\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[26.72727273 27.27272727]\n",
      " [22.27272727 22.72727273]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.545\n",
      "Precision           : 0.537\n",
      "Recall              : 0.592\n",
      "F1-score            : 0.563\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_turing('Turing1/target_v1.csv', 'Turing1/turing_jf_v1.csv')\n",
    "# filter_predictions('Turing1/target_v1.csv', 'Turing1/turing_jf_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 56 (0.56)\n",
      "Labelled synthetic  : 44 (0.44)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                47                 9\n",
      "Predicted Synthetic             3                41\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 55.5601\n",
      "p-value             : 0.0000\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[28. 28.]\n",
      " [22. 22.]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.880\n",
      "Precision           : 0.839\n",
      "Recall              : 0.940\n",
      "F1-score            : 0.887\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_turing('Turing1/target_v1.csv', 'Turing1/turing_cg_v1.csv')\n",
    "# filter_predictions('Turing1/target_v1.csv', 'Turing1/turing_cg_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 65 (0.65)\n",
      "Labelled synthetic  : 35 (0.35)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                40                25\n",
      "Predicted Synthetic            10                25\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 8.6154\n",
      "p-value             : 0.0033\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[32.5 32.5]\n",
      " [17.5 17.5]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.650\n",
      "Precision           : 0.615\n",
      "Recall              : 0.800\n",
      "F1-score            : 0.696\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_turing('Turing1/target_v1.csv', 'Turing1/turing_ms_v1.csv')\n",
    "# filter_predictions('Turing1/target_v1.csv', 'Turing1/turing_ms_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 40 (0.40)\n",
      "Labelled synthetic  : 60 (0.60)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                32                 8\n",
      "Predicted Synthetic            18                42\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 22.0417\n",
      "p-value             : 0.0000\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[20. 20.]\n",
      " [30. 30.]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.740\n",
      "Precision           : 0.800\n",
      "Recall              : 0.640\n",
      "F1-score            : 0.711\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_turing('Turing1/target_v1.csv', 'Turing1/turing_mh_v1.csv')\n",
    "# filter_predictions('Turing1/target_v1.csv', 'Turing1/turing_mh_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 52 (0.52)\n",
      "Labelled synthetic  : 48 (0.48)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                27                25\n",
      "Predicted Synthetic            23                25\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 0.0401\n",
      "p-value             : 0.8414\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[26. 26.]\n",
      " [24. 24.]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.520\n",
      "Precision           : 0.519\n",
      "Recall              : 0.540\n",
      "F1-score            : 0.529\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_turing('Turing1/target_v1.csv', 'Turing1/turing_cw_v1.csv')\n",
    "# filter_predictions('Turing1/target_v1.csv', 'Turing1/turing_cw_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate annotator agreement V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------- Overall Results\n",
      "Average Accuracy    : 0.637\n",
      "Average Precision   : 0.645\n",
      "Average Recall      : 0.660\n",
      "Average F1-score    : 0.645\n",
      "Average Human %     : 0.523\n",
      "Krippendorff Alpha  : 0.174\n",
      "Fleiss Kappa        : 0.172\n"
     ]
    }
   ],
   "source": [
    "evaluators = [\n",
    "    'Turing1/turing_ms_v1.csv',\n",
    "    'Turing1/turing_mh_v1.csv',\n",
    "    'Turing1/turing_cw_v1.csv'\n",
    "]\n",
    "\n",
    "accuracy, precision, recall, f1_score, human_percentage = [], [], [], [], []\n",
    "for e in evaluators:\n",
    "    results = evaluate_turing('Turing1/target_v1.csv', e, False)\n",
    "    accuracy.append(results[0])\n",
    "    precision.append(results[1])\n",
    "    recall.append(results[2])\n",
    "    f1_score.append(results[3])\n",
    "    human_percentage.append(results[4])\n",
    "\n",
    "overall_results = average_performance(evaluators, accuracy, precision, \n",
    "                                      recall, f1_score, human_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Common Predicted Human Actually Synthetic (5)\n",
      "replace leaking lube pump\n",
      "hmu leaking hydraulic fluid\n",
      "decking has several cracks\n",
      "boom foot clevbis pin has no grease\n",
      "diff lube hose insp for leaks\n",
      "--------------------- Common Predicted Synthetic Actually Human (4)\n",
      "replace pos 8 wheel end po\n",
      "replace faulty brake sensor1 task\n",
      "cw coolant leak from #15 cylind\n",
      "pcr room over alarm\n",
      "----------------- Common Predicted Synthetic Actually Synthetic (13)\n",
      "chge out leaking axle oil cool\n",
      "leak detected infan pump\n",
      "cabindoor is leaking\n",
      "leak in air aircon hose\n",
      "replace leaking air aircon hose\n",
      "plug has a leak\n",
      "leaikng fluid fr swingbrake pump\n",
      "gasket isleaking\n",
      "hea t ssink clamp found brnk\n",
      "machine isn't starting\n",
      "hst motor hose shows a leak\n",
      "cabin roof heater taps show leakking\n",
      "engine fan pmp hose has a leak\n",
      "------------------------- Common Predicted Human Actually Human (16)\n",
      "change out tyre pos 0\n",
      "replace leaky lh tilt cyl hose\n",
      "change out l/h lift cyl\n",
      "l/h side stick cylinder leak\n",
      "aftercooler gauge faulty\n",
      "mv3594- c/out steering ball studs\n",
      "replace bonnet hinges\n",
      "replace blown dash globes\n",
      "drag gen field exciter trips\n",
      "fw9593-fire suppression inspection\n",
      "xw7976- cw coolant leak from radiator\n",
      "major cable reroute\n",
      "2000 hr service\n",
      "repair blade tilt fault\n",
      "low transmission oil level\n",
      "repl diff cool hose to front diff\n",
      "\n",
      "Total commonly labelled: 38\n"
     ]
    }
   ],
   "source": [
    "find_common('Turing1/target_v1.csv', evaluators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turing Test 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate individual predictions V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 48 (0.48)\n",
      "Labelled synthetic  : 52 (0.52)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                31                17\n",
      "Predicted Synthetic            19                33\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 6.7708\n",
      "p-value             : 0.0093\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[24. 24.]\n",
      " [26. 26.]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.640\n",
      "Precision           : 0.646\n",
      "Recall              : 0.620\n",
      "F1-score            : 0.633\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_turing('Turing2/target_v2.csv', 'Turing2/turing_cg_v2.csv')\n",
    "# filter_predictions('Turing2/target_v2.csv', 'Turing2/turing_cg_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 49 (0.49)\n",
      "Labelled synthetic  : 51 (0.51)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                25                24\n",
      "Predicted Synthetic            25                26\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 0.0000\n",
      "p-value             : 1.0000\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[24.5 24.5]\n",
      " [25.5 25.5]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.510\n",
      "Precision           : 0.510\n",
      "Recall              : 0.500\n",
      "F1-score            : 0.505\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_turing('Turing2/target_v2.csv', 'Turing2/turing_jf_v2.csv')\n",
    "# filter_predictions('Turing2/target_v2.csv', 'Turing2/turing_jf_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 63 (0.63)\n",
      "Labelled synthetic  : 37 (0.37)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                37                26\n",
      "Predicted Synthetic            13                24\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 4.2900\n",
      "p-value             : 0.0383\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[31.5 31.5]\n",
      " [18.5 18.5]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.610\n",
      "Precision           : 0.587\n",
      "Recall              : 0.740\n",
      "F1-score            : 0.655\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_turing('Turing2/target_v2.csv', 'Turing2/turing_ms_v2.csv')\n",
    "# filter_predictions('Turing2/target_v2.csv', 'Turing2/turing_ms_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 64 (0.64)\n",
      "Labelled synthetic  : 36 (0.36)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                36                28\n",
      "Predicted Synthetic            14                22\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 2.1267\n",
      "p-value             : 0.1447\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[32. 32.]\n",
      " [18. 18.]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.580\n",
      "Precision           : 0.562\n",
      "Recall              : 0.720\n",
      "F1-score            : 0.632\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_turing('Turing2/target_v2.csv', 'Turing2/turing_mh_v2.csv')\n",
    "# filter_predictions('Turing2/target_v2.csv', 'Turing2/turing_mh_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Frequency of Labels\n",
      "Labelled human      : 53 (0.53)\n",
      "Labelled synthetic  : 47 (0.47)\n",
      "---------------------------------- Confusion Matrix\n",
      "                     Actual Human  Actual Synthetic\n",
      "Predicted Human                28                25\n",
      "Predicted Synthetic            22                25\n",
      "----------------------------------- Chi-Square Test\n",
      "Chi-square          : 0.1606\n",
      "p-value             : 0.6886\n",
      "Degrees of freedom  : 1\n",
      "Expected frequencies:\n",
      "[[26.5 26.5]\n",
      " [23.5 23.5]]\n",
      "--------------------------------------- Performance\n",
      "Accuracy            : 0.530\n",
      "Precision           : 0.528\n",
      "Recall              : 0.560\n",
      "F1-score            : 0.544\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_turing('Turing2/target_v2.csv', 'Turing2/turing_cw_v2.csv')\n",
    "# filter_predictions('Turing2/target_v2.csv', 'Turing2/turing_cw_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy with random labels: 0.49\n"
     ]
    }
   ],
   "source": [
    "# Average score if labels are randomly assigned\n",
    "# 50% accuracy is expected\n",
    "total_acc = 0\n",
    "for _ in range(100):\n",
    "    random_labels('Turing2/turing_v2.csv')\n",
    "    total_acc += evaluate_turing('Turing2/target_v2.csv', 'Turing2/turing_random.csv')[0]\n",
    "    clear_output(wait=False)\n",
    "avg_acc = total_acc / 100\n",
    "print(f'Average accuracy with random labels: {avg_acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate annotator agreement V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------- Overall Results\n",
      "Average Accuracy    : 0.573\n",
      "Average Precision   : 0.559\n",
      "Average Recall      : 0.673\n",
      "Average F1-score    : 0.610\n",
      "Average Human %     : 0.600\n",
      "Krippendorff Alpha  : 0.100\n",
      "Fleiss Kappa        : 0.097\n"
     ]
    }
   ],
   "source": [
    "evaluators = [\n",
    "    'Turing2/turing_ms_v2.csv',\n",
    "    'Turing2/turing_mh_v2.csv',\n",
    "    'Turing2/turing_cw_v2.csv'\n",
    "]\n",
    "\n",
    "accuracy, precision, recall, f1_score, human_percentage = [], [], [], [], []\n",
    "for e in evaluators:\n",
    "    results = evaluate_turing('Turing2/target_v2.csv', e, False)\n",
    "    accuracy.append(results[0])\n",
    "    precision.append(results[1])\n",
    "    recall.append(results[2])\n",
    "    f1_score.append(results[3])\n",
    "    human_percentage.append(results[4])\n",
    "\n",
    "overall_results = average_performance(evaluators, accuracy, precision, \n",
    "                                      recall, f1_score, human_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Common Predicted Human Actually Synthetic (10)\n",
      "change out w/pump sft unserviceable\n",
      "coolant pipe clamps require loctite\n",
      "d/line air conditioner not producing cold\n",
      "loctite needed on brk pump bolt\n",
      "repair crack in boom chofd\n",
      "apply loctite on clamps\n",
      "rep crack in rock deflector\n",
      "auto-lube system fault\n",
      "leak in condenser fan hose\n",
      "no grease in dogbone pin\n",
      "--------------------- Common Predicted Synthetic Actually Human (2)\n",
      "fabricate radiator stands\n",
      "clar hr air con servic\n",
      "----------------- Common Predicted Synthetic Actually Synthetic (7)\n",
      "hand is cracked\n",
      "leak found in fan pump oil\n",
      "swing brake hose is in need of rep\n",
      "parts washer pump chip sensor alarm\n",
      "boarding ladder is not wor king\n",
      "fuel leaking fr. fan pump\n",
      "hoist brk hose needs 2 be cleaned\n",
      "------------------------- Common Predicted Human Actually Human (16)\n",
      "replace bucket pins & bushes\n",
      "assisted vistacam\n",
      "1000h mech insp trk roll 2 rh\n",
      "tool inspection paul\n",
      "send tyre to mes for rebuild\n",
      "engine shut fault\n",
      "replace worn bogie pads\n",
      "repl missing boom locking pins\n",
      "repair hyd tank cover\n",
      "replace diff pressure sensor\n",
      "drain swing shaft 8 tray &clean up oil\n",
      "oil pressure gauge not working\n",
      "leak under roto\n",
      "replace a/c vent louver\n",
      "cab needs resealing\n",
      "mr0229-c/out # 75 faulty injector ass\n",
      "\n",
      "Total commonly labelled: 35\n"
     ]
    }
   ],
   "source": [
    "find_common('Turing2/target_v2.csv', evaluators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Test\n",
    "- Naturalness\n",
    "- Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 25 samples from each for ranking test\n",
    "synthetic_dirty = load_sentences(HUMANISE_DATAPATH)\n",
    "dirty_100 = synthetic_sample(synthetic_dirty, 100, 8)\n",
    "random.shuffle(dirty_100)\n",
    "dirty_100 = pd.DataFrame(dirty_100, columns=['sentence'])\n",
    "dirty_25 = random.sample(dirty_100['sentence'].tolist(), 25)\n",
    "dirty_25 = pd.DataFrame(dirty_25, columns=['sentence'])\n",
    "dirty_25['label'] = 's'\n",
    "\n",
    "human_data = load_sentences(HUMAN_DATAPATH)\n",
    "human_25 = random.sample(human_data, 25)\n",
    "human_25 = pd.DataFrame(human_25, columns=['sentence'])\n",
    "human_25['label'] = 'h'\n",
    "\n",
    "dirty_25.to_csv('dirty_25.csv', index=False)\n",
    "human_25.to_csv('human_25.csv', index=False)\n",
    "\n",
    "# Combine and shuffle human and synthetic data\n",
    "rank = pd.concat([dirty_25, human_25])\n",
    "rank = rank.sample(frac=1).reset_index(drop=True)\n",
    "rank.to_csv('rank_label.csv', index=False)  # labels\n",
    "rank['naturalness'] = ''\n",
    "rank['correctness'] = ''\n",
    "rank.drop(columns=['label']).to_csv('rank.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ranking(target, file):\n",
    "    data = pd.read_csv(file)\n",
    "    labels = pd.read_csv(target).set_index('sentence')['label']\n",
    "    results = {'Human': {'naturalness': [], 'correctness': []},\n",
    "               'Synthetic': {'naturalness': [], 'correctness': []}}\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        label = 'Synthetic' if labels[row['sentence']] == 's' else 'Human'\n",
    "        results[label]['naturalness'].append(row['naturalness'])\n",
    "        results[label]['correctness'].append(row['correctness'])\n",
    "\n",
    "    summary = {key: [np.mean(results[key]['naturalness']),\n",
    "                     np.mean(results[key]['correctness'])] for key in results}\n",
    "\n",
    "    df = pd.DataFrame(summary, index=['Naturalness', 'Correctness'])\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Human  Synthetic\n",
      "Naturalness   4.44       3.80\n",
      "Correctness   4.24       4.04\n"
     ]
    }
   ],
   "source": [
    "calculate_ranking('Rank/rank_label.csv', 'Rank/rank_mh.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
