{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\allis\\miniconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\tpaths in object_property_paths\n",
      "391\tpaths in process_agent_paths\n",
      "269\tpaths in process_patient_paths\n",
      "1208\tpaths in state_patient_paths\n",
      "3\tpaths in object_property_state_paths\n",
      "0\tpaths in object_process_state_paths\n",
      "607\tpaths in state_agent_activity_paths\n",
      "37\tpaths in state_agent_patient_paths\n",
      "1231\tpaths in process_agent_patient_paths\n",
      "Total number of paths: 3795\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from llm_generate import get_all_paths, get_generate_prompt, process_mwo_response\n",
    "from llm_prompt import initialise_prompts, check_similarity\n",
    "from llm_paraphrase import paraphrase_mwo\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "client = OpenAI(api_key=api_key)\n",
    "data, _ = get_all_paths(valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1fewshot_message(prompt_variations):\n",
    "    \"\"\" Get fewshot message given fewshot csv file for single-example prompt \"\"\"\n",
    "    message = [{\"role\": \"system\", \"content\": \"You are a technician recording maintenance work orders.\"}]\n",
    "    with open(\"fewshot_messages/fewshot_one.csv\", encoding='utf-8') as f:\n",
    "        data = csv.reader(f)\n",
    "        next(data) # Ignore header\n",
    "        for row in data:\n",
    "            object_name = row[0]\n",
    "            event_name = f\"{row[1]} {row[2]}\".strip()\n",
    "            prompt = get_generate_prompt(prompt_variations, object_name, event_name)\n",
    "            user = {\"role\": \"user\", \"content\": prompt}\n",
    "            assistant = {\"role\": \"assistant\", \"content\": row[3]}\n",
    "            message.append(user)\n",
    "            message.append(assistant)\n",
    "    return message\n",
    "\n",
    "def get_5fewshot_message(prompt_variations):\n",
    "    \"\"\" Get fewshot message given fewshot csv file for 5-example prompt \"\"\"\n",
    "    message = [{\"role\": \"system\", \"content\": \"You are a technician recording maintenance work orders.\"}]\n",
    "    with open(\"fewshot_messages/fewshot_generate.csv\", encoding='utf-8') as f:\n",
    "        data = csv.reader(f)\n",
    "        next(data) # Ignore header\n",
    "        for row in data:\n",
    "            object_name = row[0]\n",
    "            event_name = f\"{row[1]} {row[2]}\".strip()\n",
    "            prompt = get_generate_prompt(prompt_variations, object_name, event_name)\n",
    "            user = {\"role\": \"user\", \"content\": prompt}\n",
    "            example = f\"1. {row[4]}\\n2. {row[5]}\\n3. {row[6]}\\n4. {row[7]}\\n5. {row[8]}\"\n",
    "            assistant = {\"role\": \"assistant\", \"content\": example}\n",
    "            message.append(user)\n",
    "            message.append(assistant)\n",
    "    return message\n",
    "\n",
    "# Print some fewshot examples from MaintIE gold dataset\n",
    "def print_examples(object, event, helper=None):\n",
    "    \"\"\" Print some fewshot examples from the gold dataset \"\"\"\n",
    "    data = []\n",
    "    with open(\"data/MaintIE/gold_release.json\", encoding='utf-8') as f:\n",
    "        gold = json.load(f)\n",
    "        for d in gold:\n",
    "            text = d['text'].replace(\"<id> \", \"\").replace(\" <id>\", \"\")\n",
    "            data.append(text)\n",
    "    for sentence in data:\n",
    "        event_exists = re.search(rf'\\b{event}\\b', sentence)\n",
    "        helper_exists = re.search(rf'\\b{helper}\\b', sentence) if helper else None\n",
    "        if object in sentence and event_exists and helper_exists:\n",
    "            print(f\"{object},{event},{helper},{sentence}\")\n",
    "            return True\n",
    "        elif object in sentence and event_exists:\n",
    "            print(f\"{object},{event},{sentence}\")\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Process single response from the LLM\n",
    "def process_single_response(response):\n",
    "    \"\"\" Process single response from the LLM \"\"\"\n",
    "    response = response.lower()                     # Case folding\n",
    "    response = re.sub(r'[^\\w\\s]', ' ', response)    # Remove punctuation\n",
    "    response = re.sub(r\"\\s+\", \" \", response)        # Remove extra spaces\n",
    "    return response\n",
    "\n",
    "# write to outlog text file\n",
    "def write_to_outlog(title, data):\n",
    "    \"\"\" Write data to log text file \"\"\"\n",
    "    with open(\"mwo_sentences/path_diversity.txt\", \"a\", encoding='utf-8') as f:\n",
    "        f.write(\"========================================\\n\")\n",
    "        f.write(f\"{title}\\n\")\n",
    "        f.write(f\"- Number of unique: {len(data)}\\n\")\n",
    "        for d in data:\n",
    "            f.write(f\"{d}\\n\")\n",
    "        f.write(\"========================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steering column,movement,movement on steering column\n",
      "aftercooler,error,aftercooler temperature error\n",
      "brake,pressure fault,swing brake pressure fault\n",
      "air conditioner,needs,boost,air conditioner needs a boost not cold\n",
      "track frame,crack,- repair right hand track frame crack\n"
     ]
    }
   ],
   "source": [
    "# Print some examples from the gold dataset\n",
    "successful_calls = 0\n",
    "while successful_calls < 5:\n",
    "    current = random.choice(data)\n",
    "    if 'helper_name' in current:\n",
    "        if print_examples(current['object_name'], current['event_name'], current['helper_name']):\n",
    "            successful_calls += 1\n",
    "    else:\n",
    "        if print_examples(current['object_name'], current['event_name']):\n",
    "            successful_calls += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same prompt VS Variant prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\allis\\miniconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a Maintenance Work Order (MWO) sentence detailing the equipment and the undesirable event.\n",
      "Write a sentence for a Maintenance Work Order (MWO) referring to the equipment and the undesirable event.\n",
      "Compose a sentence for a Maintenance Work Order (MWO) that specifies the equipment and the undesirable event.\n",
      "Draft a sentence for a Maintenance Work Order (MWO) that outlines the equipment and the undesirable event.\n",
      "Formulate a sentence for a Maintenance Work Order (MWO) that describes the equipment and the undesirable event.\n",
      "Avoid verbosity and minimize stop words.\n",
      "Don't use verbosity; keep stop words to a minimum.\n",
      "Refrain from verbosity and limit stop words.\n",
      "Eliminate verbosity and reduce stop words.\n",
      "Skip verbosity and cut down on stop words.\n",
      "The sentence can only be 8 words long.\n",
      "A sentence must have a maximum of 8 words.\n",
      "The sentence can contain up to 8 words.\n",
      "Ensure the sentence is 8 words or fewer.\n",
      "The sentence should not exceed 8 words.\n"
     ]
    }
   ],
   "source": [
    "path = {\n",
    "        \"object_name\": \"park brake\",\n",
    "        \"event_name\": \"alarm fault\",\n",
    "        \"valid\": True,\n",
    "        \"alternate\": False\n",
    "    }\n",
    "prompt_variations = initialise_prompts(client, num_variants=5, num_examples=1)\n",
    "base_prompts, limit_words, limit_count = prompt_variations\n",
    "for b in base_prompts:\n",
    "    print(b)\n",
    "for w in limit_words:\n",
    "    print(w)\n",
    "for c in limit_count:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same prompt\n",
    "def same_prompt():\n",
    "    outputs = []\n",
    "    base_prompt = \"Generate a Maintenance Work Order (MWO) sentence describing the following equipment and undesirable event.\"\n",
    "    limit_words = \"Avoid verbosity and use minimal stop words.\"\n",
    "    limit_count = \"The sentence can have a maximum of 8 words.\"\n",
    "    prompt = f\"{base_prompt}\\nEquipment: {path['object_name']}\\nUndesirable Event: {path['event_name']}\"\n",
    "    prompt += f\"You must use all terms given above and do not add new information.\\n{limit_words}\\n{limit_count}\"\n",
    "    fewshot = get_1fewshot_message(([base_prompt], [limit_words], [limit_count]))\n",
    "    message = fewshot + [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=message,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.9,\n",
    "                    n=5\n",
    "                )\n",
    "\n",
    "    for choice in response.choices:\n",
    "        output = choice.message.content\n",
    "        output = process_single_response(output) \n",
    "        outputs.append(output)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant prompts\n",
    "def variant_prompt():\n",
    "    outputs = []\n",
    "    fewshot = get_1fewshot_message(prompt_variations)\n",
    "    for _ in range(5):\n",
    "        prompt = get_generate_prompt(prompt_variations, path['object_name'], path['event_name'])\n",
    "        message = fewshot + [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = client.chat.completions.create(\n",
    "                        model=\"gpt-4o-mini\",\n",
    "                        messages=message,\n",
    "                        temperature=0.9,\n",
    "                        top_p=0.9,\n",
    "                        n=1\n",
    "                )\n",
    "\n",
    "        for choice in response.choices:\n",
    "            output = choice.message.content\n",
    "            output = process_single_response(output)\n",
    "            outputs.append(output)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT given same prompt:\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault\n",
      "- park brake alarm fault\n",
      "- park brake alarm fault\n",
      "GPT given variant prompts:\n",
      "- park brake has alarm fault\n",
      "- park brake has alarm fault\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault\n"
     ]
    }
   ],
   "source": [
    "# Demo\n",
    "same = same_prompt()\n",
    "variant = variant_prompt()\n",
    "\n",
    "print (\"GPT given same prompt:\")\n",
    "for s in same:\n",
    "    print(f\"- {s}\")\n",
    "print (\"GPT given variant prompts:\")\n",
    "for v in variant:\n",
    "    print(f\"- {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique responses for same prompt: 3\n",
      "Number of unique responses for variant prompts: 3\n",
      "Total number of unique responses: 3\n"
     ]
    }
   ],
   "source": [
    "# Compare diversity over long run\n",
    "same = []\n",
    "variant = []\n",
    "for _ in range(20):\n",
    "    same.extend(same_prompt())\n",
    "    variant.extend(variant_prompt())\n",
    "print (f\"Number of unique responses for same prompt: {len(set(same))}\")\n",
    "print (f\"Number of unique responses for variant prompts: {len(set(variant))}\")\n",
    "write_to_outlog(\"OUTPUT FOR SAME PROMPT\", list(set(same)))\n",
    "write_to_outlog(\"OUTPUT FOR VARIANT PROMPTS\", list(set(variant)))\n",
    "total = same + variant\n",
    "print (f\"Total number of unique responses: {len(set(total))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N completions VS N sentences in ONE completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N sentences in ONE completion\n",
    "def n_sentences():\n",
    "    outputs = []\n",
    "    base_prompt = \"Generate 5 different Maintenance Work Order (MWO) sentences describing the following equipment and undesirable event.\"\n",
    "    limit_words = \"Avoid verbosity and use minimal stop words.\"\n",
    "    limit_count = \"Each sentence can have a maximum of 8 words.\"\n",
    "    prompt = f\"{base_prompt}\\nEquipment: {path['object_name']}\\nUndesirable Event: {path['event_name']}\"\n",
    "    prompt += f\"You must use all terms given above and do not add new information.\\n{limit_words}\\n{limit_count}\"\n",
    "    fewshot = get_5fewshot_message(([base_prompt], [limit_words], [limit_count]))\n",
    "    message = fewshot + [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=message,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.9,\n",
    "                    n=1\n",
    "                )\n",
    "\n",
    "    for choice in response.choices:\n",
    "        output = choice.message.content\n",
    "        output = process_mwo_response(output)\n",
    "        for sentence in output:\n",
    "            outputs.append(sentence)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output in N completions:\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault\n",
      "- park brake alarm fault\n",
      "- park brake alarm fault\n",
      "- park brake alarm fault detected\n",
      "Output within 1 completion:\n",
      "- park brake alarm fault\n",
      "- alarm fault in park brake\n",
      "- park brake has alarm fault\n",
      "- alarm fault found in park brake\n",
      "- park brake showing alarm fault\n"
     ]
    }
   ],
   "source": [
    "# Demo\n",
    "n_completion = same_prompt()\n",
    "n_sentence = n_sentences()\n",
    "\n",
    "print (\"Output in N completions:\")\n",
    "for c in n_completion:\n",
    "    print(f\"- {c}\")\n",
    "print (\"Output within 1 completion:\")\n",
    "for s in n_sentence:\n",
    "    print(f\"- {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique responses for N completions: 2\n",
      "Number of unique responses for ONE completion: 15\n",
      "Total number of unique responses: 15\n"
     ]
    }
   ],
   "source": [
    "# Compare diversity over long run\n",
    "n_completion = []\n",
    "n_sentence = []\n",
    "for _ in range(20):\n",
    "    n_completion.extend(same_prompt())\n",
    "    n_sentence.extend(n_sentences())\n",
    "print (f\"Number of unique responses for N completions: {len(set(n_completion))}\")\n",
    "print (f\"Number of unique responses for ONE completion: {len(set(n_sentence))}\")\n",
    "write_to_outlog(\"OUTPUT IN N COMPLETIONS\", list(set(n_completion)))\n",
    "write_to_outlog(\"OUTPUT IN 1 COMPLETION\", list(set(n_sentence)))\n",
    "total_responses = n_completion + n_sentence\n",
    "print (f\"Total number of unique responses: {len(set(total_responses))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrase VS Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paraphrase\n",
    "def paraphrase():\n",
    "    outputs = []\n",
    "    base_prompt = \"Generate a Maintenance Work Order (MWO) sentence describing the following equipment and undesirable event.\"\n",
    "    limit_words = \"Avoid verbosity and use minimal stop words.\"\n",
    "    limit_count = \"Each sentence can have a maximum of 8 words.\"\n",
    "    prompt = f\"{base_prompt}\\nEquipment: {path['object_name']}\\nUndesirable Event: {path['event_name']}\"\n",
    "    prompt += f\"You must use all terms given above and do not add new information.\\n{limit_words}\\n{limit_count}\"\n",
    "    fewshot = get_1fewshot_message(([base_prompt], [limit_words], [limit_count]))\n",
    "    message = fewshot + [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=message,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.9,\n",
    "                    n=1\n",
    "            )\n",
    "\n",
    "    for choice in response.choices:\n",
    "        output = choice.message.content\n",
    "        output = process_single_response(output)\n",
    "        outputs.append(output)\n",
    "\n",
    "    keywords = [path['object_name'], path['event_name']]\n",
    "    paraphrases = paraphrase_mwo(client, output, keywords, 5)\n",
    "    outputs.extend(paraphrases)\n",
    "    outputs = list(set(outputs)) # Remove duplicates\n",
    "    return outputs[:5] # Only return 5 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for Paraphrasing:\n",
      "- detected alarm fault in park brake\n",
      "- park brake alarm fault detected\n",
      "- park brake has alarm fault\n",
      "- alarm fault detected in park brake\n",
      "- alarm fault found in park brake\n",
      "Output for Generation:\n",
      "- park brake alarm fault\n",
      "- alarm fault in park brake\n",
      "- park brake has alarm fault\n",
      "- park brake alarm is faulty\n",
      "- faulty alarm in park brake\n"
     ]
    }
   ],
   "source": [
    "# Demo\n",
    "paraphrases = paraphrase()\n",
    "n_sentence = n_sentences()\n",
    "\n",
    "print (\"Output for Paraphrasing:\")\n",
    "for p in paraphrases:\n",
    "    print(f\"- {p}\")\n",
    "print (\"Output for Generation:\")\n",
    "for s in n_sentence:\n",
    "    print(f\"- {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique responses for paraphrasing: 25\n",
      "Number of unique responses for generation: 16\n",
      "Total unique responses: 32\n"
     ]
    }
   ],
   "source": [
    "# Compare diversity over long run\n",
    "paraphrases = []\n",
    "n_sentence = []\n",
    "for _ in range(20):\n",
    "    paraphrases.extend(paraphrase())\n",
    "    n_sentence.extend(n_sentences())\n",
    "print (f\"Number of unique responses for paraphrasing: {len(set(paraphrases))}\")\n",
    "print (f\"Number of unique responses for generation: {len(set(n_sentence))}\")\n",
    "write_to_outlog(\"OUTPUT FOR PARAPHRASING\", list(set(paraphrases)))\n",
    "write_to_outlog(\"OUTPUT FOR GENERATION\", list(set(n_sentence)))\n",
    "total_responses = paraphrases + n_sentence\n",
    "print (f\"Total unique responses: {len(set(total_responses))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Paraphraser Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def format_data():\n",
    "    finetune_data = []\n",
    "    with open('fewshot_messages/fewshot.csv', 'r', encoding='utf-8') as f:\n",
    "        data = csv.reader(f)\n",
    "        next(data) # Ignore header\n",
    "        for row in data:\n",
    "            if len(row) > 4:\n",
    "                original = row[3]\n",
    "                example = [row[4], row[5], row[6], row[7]]\n",
    "                for e in example:\n",
    "                    temp = {\n",
    "                        \"source\": original, \n",
    "                        \"target\": e\n",
    "                    }\n",
    "                    finetune_data.append(temp)\n",
    "                    \n",
    "    finetune_train = finetune_data[:int(0.8*len(finetune_data))]\n",
    "    finetune_val = finetune_data[int(0.8*len(finetune_data)):]\n",
    "    with open('fewshot_messages/train.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(finetune_train, f, indent=4)\n",
    "    with open('fewshot_messages/val.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(finetune_val, f, indent=4)\n",
    "\n",
    "# Transformer model for paraphrasing\n",
    "def model_paraphrase(model, tokenizer, sentence):\n",
    "    input_ids = tokenizer(\n",
    "        f'paraphrase: {sentence}',\n",
    "        return_tensors=\"pt\", padding=\"longest\",\n",
    "        max_length=25, truncation=True).input_ids.to(device)\n",
    "    outputs = model.generate(\n",
    "        input_ids, temperature=0.7, repetition_penalty=10.0,\n",
    "        num_return_sequences=5, no_repeat_ngram_size=2,\n",
    "        num_beams=5, num_beam_groups=5,\n",
    "        max_length=25, diversity_penalty=3.0\n",
    "    )\n",
    "    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return res\n",
    "\n",
    "def paraphrase_sentences(tokenizer, model, sentences):\n",
    "    outputs = {}\n",
    "    for sentence in sentences:\n",
    "        batch = tokenizer([sentence],truncation=True,padding='longest',max_length=25, return_tensors=\"pt\").to(device)\n",
    "        paraphrased = model.generate(**batch, temperature=0.7, repetition_penalty=10.0,\n",
    "                                        num_return_sequences=5, no_repeat_ngram_size=2,\n",
    "                                        num_beams=5, num_beam_groups=5,\n",
    "                                        max_length=25, diversity_penalty=3.0\n",
    "                                    )\n",
    "        output = tokenizer.batch_decode(paraphrased, skip_special_tokens=True)\n",
    "        outputs[sentence] = output\n",
    "    return outputs\n",
    "\n",
    "format_data()\n",
    "\n",
    "sentences = [\"park brake alarm fault detected\",\n",
    "             \"alarm fault in park brake system\",\n",
    "             \"park brake has alarm fault\",\n",
    "             \"park brake system shows alarm fault\",\n",
    "             \"alarm fault present in park brake\",\n",
    "             \"The boy is walking happily on the street\"]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART Paraphrase Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART Outputs:\n",
      "park brake alarm fault detected:\n",
      "- Park brake alarm fault detected\n",
      "- The alarm fault was detected in the park brake.\n",
      "- park Brake alarm fault detected.\n",
      "- Brake brake alarm Fault detected\n",
      "- Park brake alarm Fault detect\n",
      "alarm fault in park brake system:\n",
      "- Alarm Fault in park brake system?\n",
      "- What is the cause of an alarm fault in a park brake system?\n",
      "- The alarm fault is in park brake system.\n",
      "- In park brake system, alarm fault is not fixed.\n",
      "- System Fault in park brake system?\n",
      "park brake has alarm fault:\n",
      "- Park brake has alarm fault. How can it be fixed?\n",
      "- The park brake has alarm fault. How can it be fixed?\n",
      "- What is the cause of a brake alarm fault?\n",
      "- Park brakes have an alarm fault. What should I do to fix it?\n",
      "- park brake has alarm faults, it is not working.\n",
      "park brake system shows alarm fault:\n",
      "- Park brake system shows alarm fault.\n",
      "- The park brake system shows alarm fault.\n",
      "- Park brakes show alarm fault. How can this be fixed?\n",
      "- A parking brake system shows an alarm fault. How can this be fixed?\n",
      "- park brake system shows alarm faults, says the company.\n",
      "alarm fault present in park brake:\n",
      "- Alarm fault present in park brake?\n",
      "- In park brake, there is an alarm fault.\n",
      "- What are the symptoms of an alarm fault in a park brake?\n",
      "- There is an alarm fault in park brake.\n",
      "- The alarm fault in park brake is present.\n",
      "The boy is walking happily on the street:\n",
      "- The boy is happily walking on the street.\n",
      "- He is happily walking on the street.\n",
      "- A young boy is happily walking on the street.\n",
      "- Mr. Williams, who is in his early 20s, is happily walking on the street.\n",
      "- The boy is walking happily in the road, he said.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/eugenesiow/bart-paraphrase\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('eugenesiow/bart-paraphrase')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('eugenesiow/bart-paraphrase')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bart_model = bart_model.to(device)\n",
    "\n",
    "bart_outputs = paraphrase_sentences(bart_tokenizer, bart_model, sentences)\n",
    "\n",
    "print(\"BART Outputs:\")\n",
    "for sentence, outputs in bart_outputs.items():\n",
    "    print(f\"{sentence}:\")\n",
    "    for output in outputs:\n",
    "        print(f\"- {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Paraphrase Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "park brake alarm fault detected:\n",
      "- A park brake warning error has been found.\n",
      "- An accident has been reported with the park brake alert.\n",
      "- There has been a problem with the park brake alert system.\n",
      "- A parking brake alarm fault was detected.\n",
      "- The park brake alert system is running fine, but it has been detected.\n",
      "alarm fault in park brake system:\n",
      "- An alarm is raised in the park brake system.\n",
      "- An alarm system in the park brake system is failing.\n",
      "- There is an error in the park brake system that causes this alert.\n",
      "- an error occurred in the park brake system, causing an alarm.\n",
      "- In the park brake system, an alarm clock malfunctions.\n",
      "park brake has alarm fault:\n",
      "- The park brake has an alarm clock fault.\n",
      "- The park brake has an error, according to the driver.\n",
      "- A security issue with the park brake has been fixed.\n",
      "- An alarm is triggered by the park brake.\n",
      "- If the park brake has an error, it is possible to fix it.\n",
      "park brake system shows alarm fault:\n",
      "- The park brake system is showing an error.\n",
      "- An alarm clock is detected on the park brake system.\n",
      "- The park brake system sounds an alarm clock failure.\n",
      "- A warning light on the park brake system has been flashing.\n",
      "- An alarm system has failed in the park brake unit.\n",
      "alarm fault present in park brake:\n",
      "- In the park brake, an alarm clock malfunction is present.\n",
      "- An alarm clock fault is present in the park brake.\n",
      "- The park brake has an alarm system that is malfunctioning.\n",
      "- There is an alarm fault in the park brake system.\n",
      "- A warning light is triggered by the park brake, which is present in the vehicle.\n",
      "The boy is walking happily on the street:\n",
      "- The boy is walking happily on the street.\n",
      "- The child is enjoying his time on the street.\n",
      "- The little boy is walking happily on the pavement.\n",
      "- On the street, the boy is enjoying himself while walking happily.\n",
      "- He is walking happily on the street.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/ramsrigouthamg/t5_sentence_paraphraser\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "\n",
    "t5_sent_model = T5ForConditionalGeneration.from_pretrained(\"ramsrigouthamg/t5_sentence_paraphraser\")\n",
    "t5_sent_tokenizer = T5Tokenizer.from_pretrained(\"ramsrigouthamg/t5_sentence_paraphraser\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "t5_sent_model = t5_sent_model.to(device)\n",
    "\n",
    "for sentence in sentences:\n",
    "    outputs = model_paraphrase(t5_sent_model, t5_sent_tokenizer, sentence)\n",
    "    print(f\"{sentence}:\")\n",
    "    for output in outputs:\n",
    "        print(f\"- {output.removeprefix('paraphrasedoutput: ')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT Paraphrase T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "park brake alarm fault detected:\n",
      "- A fault in the park brake alarm has been identified.\n",
      "- Detection of a fault in the park brake alarm system\n",
      "- The park brake alarm has been deemed defective.\n",
      "- An error has been identified in the park brake alarm's functionality.\n",
      "- Park brake alarm faulty, please.\n",
      "alarm fault in park brake system:\n",
      "- An alarm failure has been detected in the park brake system.\n",
      "- The park brake system has been deemed to be defective due to an\n",
      "- A faulty alarm in the parking brake system has been detected.\n",
      "- There is an alarm failure in the park brake system.\n",
      "- Park brakes issue alarm\n",
      "park brake has alarm fault:\n",
      "- The park brake is experiencing an alarm failure.\n",
      "- Alarm jamming on park brake\n",
      "- Park brake with alarm faulty.\n",
      "- An alarm is malfunctioning on the park brake.\n",
      "- Alarm caliper failing on park brake.\n",
      "park brake system shows alarm fault:\n",
      "- The alarm is malfunctioning on the park brake system.\n",
      "- Alarm triggered by park brake system.\n",
      "- Alarm failure occurs in the parking brake system.\n",
      "- The park brake system is malfunctioning and the alarm has been activated\n",
      "- An alarm failure is causing the brakes to stop turning on the\n",
      "alarm fault present in park brake:\n",
      "- A fault in the park brakes is causing an alarm failure.\n",
      "- An alarm malfunction was observed in the park brakes.\n",
      "- The park brakes are experiencing an alarm failure.\n",
      "- Park brake issue with alarm.\n",
      "- There is an alarm failure on the parking brakes.\n",
      "The boy is walking happily on the street:\n",
      "- The boy is enjoying himself on the street.\n",
      "- With a smile on his face, the boy strolls along the\n",
      "- A happy boy walks along the street\n",
      "- Happy on the street: The boy walks along happily.\n",
      "- The boy walks in a cheerful manner on the street.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "chat_model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "chat_model = chat_model.to(device)\n",
    "\n",
    "for sentence in sentences:\n",
    "    outputs = model_paraphrase(chat_model, chat_tokenizer, sentence)\n",
    "    print(f\"{sentence}:\")\n",
    "    for output in outputs:\n",
    "        print(f\"- {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parrot Paraphrase Model T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "park brake alarm fault detected\n",
      "- ('park brake alarm fault detected', 0)\n",
      "alarm fault in park brake system\n",
      "- ('warning of a fault in the brake system', 17)\n",
      "park brake has alarm fault\n",
      "No paraphrases found.\n",
      "park brake system shows alarm fault\n",
      "- ('the park brakes system shows an alarm fault', 13)\n",
      "alarm fault present in park brake\n",
      "- ('warning of a fault in the park brake', 22)\n",
      "The boy is walking happily on the street\n",
      "- ('a happy boy walks along the streets', 26)\n",
      "- ('he walks happily on the street', 23)\n",
      "- ('the boy walks happily in the street', 19)\n",
      "- ('the boy walks happily on the street', 18)\n",
      "- ('the boy is walking happily on the street', 12)\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/prithivida/parrot_paraphraser_on_T5\n",
    "from parrot import Parrot\n",
    "\n",
    "parrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\", use_gpu=False)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    paraphrases = parrot.augment(input_phrase=sentence,\n",
    "                                diversity_ranker=\"levenshtein\",\n",
    "                                do_diverse=True, \n",
    "                                max_return_phrases=5, \n",
    "                                max_length=8)\n",
    "    if paraphrases:\n",
    "        for p in paraphrases:\n",
    "            print(f\"- {p}\")\n",
    "    else:\n",
    "        print(\"No paraphrases found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEGASUS fine-tuned for Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "park brake alarm fault detected:\n",
      "- The park brake alarm fault was detected.\n",
      "- Park brake alarm fault detected.\n",
      "- A fault with the park brake alarm has been detected.\n",
      "- There is a park brake alarm fault\n",
      "- Park brakes fault detected.\n",
      "alarm fault in park brake system:\n",
      "- There is an alarm fault in the park brake system.\n",
      "- The alarm fault in the park brake system.\n",
      "- Park brake system has an alarm fault.\n",
      "- It was a fault in the park brake system.\n",
      "- A fault in the park brake system was cited.\n",
      "park brake has alarm fault:\n",
      "- The park brake has an alarm fault.\n",
      "- Park brake has an alarm fault.\n",
      "- There is an alarm fault with the park brake.\n",
      "- The alarm fault is on the park brake.\n",
      "- A park brake has a problem\n",
      "park brake system shows alarm fault:\n",
      "- The park brake system has an alarm fault.\n",
      "- Park brake system shows alarm fault.\n",
      "- There is an alarm fault in the park brake system.\n",
      "- A park brakes alarm shows a fault.\n",
      "- The Park Brake System shows alarm fault\n",
      "alarm fault present in park brake:\n",
      "- There is an alarm fault in the park brake.\n",
      "- A fault in the park brake.\n",
      "- It was a fault in the park brake.\n",
      "- alarm fault present in the park brake\n",
      "- The alarm fault is present in the park brakes.\n",
      "The boy is walking happily on the street:\n",
      "- The boy is walking happily.\n",
      "- There is a boy walking on the street.\n",
      "- A boy walks happily on the street.\n",
      "- The child is on the street.\n",
      "- the kid walked happily on a road\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/tuner007/pegasus_paraphrase\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pegasus_model = pegasus_model.to(device)\n",
    "\n",
    "for sentence in sentences:\n",
    "    outputs = model_paraphrase(pegasus_model, pegasus_tokenizer, sentence)\n",
    "    print(f\"{sentence}:\")\n",
    "    for output in outputs:\n",
    "        print(f\"- {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Generation Process\n",
    "For generating sentences for ONE path, we can use the following process:\n",
    "1. Use multiple **prompt variations**\n",
    "2. Generate **multiple sentences** for each completion using a random prompt\n",
    "3. Generate **paraphrases** for each generated sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 25 paths\n",
      "Completed 50 paths\n",
      "Completed 75 paths\n",
      "Completed 100 paths\n"
     ]
    }
   ],
   "source": [
    "# Generate 100 random sentences using diverse\n",
    "base_prompts, limit_words, limit_count = initialise_prompts(client, num_variants=5, num_examples=5)\n",
    "paths_sample = random.sample(data, 100)\n",
    "sentences_sample = []\n",
    "\n",
    "# Generate sentences for paths\n",
    "for i, path in enumerate(paths_sample):\n",
    "    # Variant prompt\n",
    "    prompt = get_generate_prompt(base_prompts, limit_words, limit_count, path['object_name'], path['event_name'])\n",
    "    keywords = [path['object_name'], path['event_name']]\n",
    "    fewshot = get_5fewshot_message(base_prompts, limit_words, limit_count)\n",
    "    message = fewshot + [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Generation\n",
    "    sentences = []\n",
    "    response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=message,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.9,\n",
    "                    n=1\n",
    "                )\n",
    "    response_sentences = process_mwo_response(response.choices[0].message.content)\n",
    "    sentences.extend(response_sentences)\n",
    "    sentences = list(set(sentences)) # Remove duplicates\n",
    "\n",
    "    # Paraphrase\n",
    "    paraphrases = []\n",
    "    chosen_sentence = random.choice(sentences)\n",
    "    response_paraphrases = paraphrase_mwo(client, chosen_sentence, keywords, 5)\n",
    "    response_similarities = check_similarity(chosen_sentence, response_paraphrases)\n",
    "    for para, sim in zip(response_paraphrases, response_similarities):\n",
    "        if sim > 0.9:\n",
    "            paraphrases.append(para)\n",
    "    paraphrases = list(set(paraphrases)) # Remove duplicates\n",
    "\n",
    "    # Combine\n",
    "    sentences.extend(paraphrases)\n",
    "    sentences = list(set(sentences)) # Remove duplicates\n",
    "    sentences_sample.append(random.choice(sentences))\n",
    "    \n",
    "    # Print progress status every 25 paths\n",
    "    if (i+1) % 25 == 0:\n",
    "        print(f\"Completed {i+1} paths\")\n",
    "\n",
    "# Append to text file\n",
    "with open('TuringTest/synthetic_generate.txt', 'a', encoding='utf-8') as f:\n",
    "    for sentence in sentences_sample:\n",
    "        f.write(f\"{sentence}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
