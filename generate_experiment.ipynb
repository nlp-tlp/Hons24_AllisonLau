{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\allis\\miniconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\tpaths in object_property_paths\n",
      "567\tpaths in process_agent_paths\n",
      "402\tpaths in process_patient_paths\n",
      "1743\tpaths in state_patient_paths\n",
      "7\tpaths in object_property_state_paths\n",
      "0\tpaths in object_process_state_paths\n",
      "202\tpaths in state_agent_activity_paths\n",
      "44\tpaths in state_agent_patient_paths\n",
      "188\tpaths in process_agent_patient_paths\n",
      "Total number of paths: 3207\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from llm_generation import get_all_paths\n",
    "from llm_paraphrase import initialise_prompts, get_generate_prompt, paraphrase_mwo, process_mwo_response, check_similarity\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "client = OpenAI(api_key=api_key)\n",
    "data = get_all_paths(valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1fewshot_message(base_prompts, instructions):\n",
    "    \"\"\" Get fewshot message given fewshot csv file for single-example prompt \"\"\"\n",
    "    message = [{\"role\": \"system\", \"content\": \"You are a technician recording maintenance work orders.\"}]\n",
    "    with open(\"fewshot_messages/fewshot.csv\", encoding='utf-8') as f:\n",
    "        data = csv.reader(f)\n",
    "        next(data) # Ignore header\n",
    "        # Single-example prompt\n",
    "        for row in data:\n",
    "            if len(row) == 4:\n",
    "                if row[2] == \"\": # No helper\n",
    "                    user = {\"role\": \"user\", \"content\": get_generate_prompt(base_prompts, instructions, row[0], row[1])}\n",
    "                else:            # Has helper\n",
    "                    user = {\"role\": \"user\", \"content\": get_generate_prompt(base_prompts, instructions, row[0], row[1], row[2])}\n",
    "                assistant = {\"role\": \"assistant\", \"content\": row[3]}\n",
    "                message.append(user)\n",
    "                message.append(assistant)   \n",
    "    # Save fewshot message to json file\n",
    "    with open(\"fewshot_messages/fewshot1.json\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(message, f, indent=4)\n",
    "    return message\n",
    "\n",
    "def get_5fewshot_message(base_prompts, instructions):\n",
    "    \"\"\" Get fewshot message given fewshot csv file for 5-example prompt \"\"\"\n",
    "    message = [{\"role\": \"system\", \"content\": \"You are a technician recording maintenance work orders.\"}]\n",
    "    with open(\"fewshot_messages/fewshot.csv\", encoding='utf-8') as f:\n",
    "        data = csv.reader(f)\n",
    "        next(data) # Ignore header\n",
    "        # 5-example prompt\n",
    "        for row in data:\n",
    "            if len(row) > 4:\n",
    "                if row[2] == \"\": # No helper\n",
    "                    user = {\"role\": \"user\", \"content\": get_generate_prompt(base_prompts, instructions, row[0], row[1])}\n",
    "                else:\n",
    "                    user = {\"role\": \"user\", \"content\": get_generate_prompt(base_prompts, instructions, row[0], row[1], row[2])}\n",
    "                example = f\"1. {row[3]}\\n2. {row[4]}\\n3. {row[5]}\\n4. {row[6]}\\n5. {row[7]}\"\n",
    "                assistant = {\"role\": \"assistant\", \"content\": example}\n",
    "                message.append(user)\n",
    "                message.append(assistant)\n",
    "    # Save fewshot message to json file\n",
    "    with open(\"fewshot_messages/fewshot5.json\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(message, f, indent=4)\n",
    "    return message\n",
    "\n",
    "\n",
    "# Print some fewshot examples from MaintIE gold dataset\n",
    "def print_examples(object, event, helper=None):\n",
    "    \"\"\" Print some fewshot examples from the gold dataset \"\"\"\n",
    "    data = []\n",
    "    with open(\"data/MaintIE/gold_release.json\", encoding='utf-8') as f:\n",
    "        gold = json.load(f)\n",
    "        for d in gold:\n",
    "            text = d['text'].replace(\"<id> \", \"\").replace(\" <id>\", \"\")\n",
    "            data.append(text)\n",
    "    for sentence in data:\n",
    "        event_exists = re.search(rf'\\b{event}\\b', sentence)\n",
    "        helper_exists = re.search(rf'\\b{helper}\\b', sentence) if helper else None\n",
    "        if object in sentence and event_exists and helper_exists:\n",
    "            print(f\"{object},{event},{helper},{sentence}\")\n",
    "            return True\n",
    "        elif object in sentence and event_exists:\n",
    "            print(f\"{object},{event},{sentence}\")\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Process single response from the LLM\n",
    "def process_single_response(response):\n",
    "    \"\"\" Process single response from the LLM \"\"\"\n",
    "    response = response.lower()                     # Case folding\n",
    "    response = re.sub(r'[^\\w\\s]', ' ', response)    # Remove punctuation\n",
    "    response = re.sub(r\"\\s+\", \" \", response)        # Remove extra spaces\n",
    "    return response\n",
    "\n",
    "# write to outlog text file\n",
    "def write_to_outlog(title, data):\n",
    "    \"\"\" Write data to outlog text file \"\"\"\n",
    "    with open(\"outlog.txt\", \"a\", encoding='utf-8') as f:\n",
    "        f.write(\"========================================\\n\")\n",
    "        f.write(f\"{title}\\n\")\n",
    "        for d in data:\n",
    "            f.write(f\"{d}\\n\")\n",
    "        f.write(\"========================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air brake line,requires,replacing,air brake line blown - requires replacing\n",
      "bucket cylinder hose,blown,blown right hand bucket cylinder hose\n",
      "hydraulic hose,leaking,change out leaking hydraulic hose\n",
      "chain,fault,drag chain fault\n",
      "tank gauge,broken,aftercooler tank gauge broken\n"
     ]
    }
   ],
   "source": [
    "# Print some examples from the gold dataset\n",
    "successful_calls = 0\n",
    "while successful_calls < 5:\n",
    "    current = random.choice(data)\n",
    "    if 'helper_name' in current:\n",
    "        if print_examples(current['object_name'], current['event_name'], current['helper_name']):\n",
    "            successful_calls += 1\n",
    "    else:\n",
    "        if print_examples(current['object_name'], current['event_name']):\n",
    "            successful_calls += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same prompt VS Variant prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "path = random.choice(data)\n",
    "base_prompts, instructions = initialise_prompts(client, num_variants=5, num_examples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same prompt\n",
    "def same_prompt():\n",
    "    outputs = []\n",
    "    base_prompt = \"Generate a Maintenance Work Order (MWO) sentence describing the following equipment and undesirable event.\"\n",
    "    instruction = \"The sentence can have a maximum of 8 words.\"\n",
    "    prompt = f\"{base_prompt}\\nEquipment: {path['object_name']}\\nUndesirable Event: {path['event_name']}\\n{instruction}\"\n",
    "    fewshot = get_1fewshot_message([base_prompt], [instruction])\n",
    "    message = fewshot + [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=message,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.9,\n",
    "                    n=5\n",
    "                )\n",
    "\n",
    "    for choice in response.choices:\n",
    "        output = choice.message.content\n",
    "        output = process_single_response(output) \n",
    "        outputs.append(output)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant prompts\n",
    "def variant_prompt():\n",
    "    outputs = []\n",
    "    fewshot = get_1fewshot_message(base_prompts, instructions)\n",
    "    for _ in range(5):\n",
    "        prompt = get_generate_prompt(base_prompts, instructions, path['object_name'], path['event_name'])\n",
    "        message = fewshot + [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = client.chat.completions.create(\n",
    "                        model=\"gpt-4o-mini\",\n",
    "                        messages=message,\n",
    "                        temperature=0.9,\n",
    "                        top_p=0.9,\n",
    "                        n=1\n",
    "                )\n",
    "\n",
    "        for choice in response.choices:\n",
    "            output = choice.message.content\n",
    "            output = process_single_response(output)\n",
    "            outputs.append(output)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT given same prompt:\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "GPT given variant prompts:\n",
      "- park brake alarm fault detected\n",
      "- park brake has alarm fault\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault needs attention\n",
      "- park brake shows alarm fault\n"
     ]
    }
   ],
   "source": [
    "# Demo\n",
    "same = same_prompt()\n",
    "variant = variant_prompt()\n",
    "\n",
    "print (\"GPT given same prompt:\")\n",
    "for s in same:\n",
    "    print(f\"- {s}\")\n",
    "print (\"GPT given variant prompts:\")\n",
    "for v in variant:\n",
    "    print(f\"- {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique responses for same prompt: 3\n",
      "Number of unique responses for variant prompts: 5\n"
     ]
    }
   ],
   "source": [
    "# Compare diversity over long run\n",
    "same = []\n",
    "variant = []\n",
    "for _ in range(20):\n",
    "    same.extend(same_prompt())\n",
    "    variant.extend(variant_prompt())\n",
    "print (f\"Number of unique responses for same prompt: {len(set(same))}\")\n",
    "print (f\"Number of unique responses for variant prompts: {len(set(variant))}\")\n",
    "write_to_outlog(\"Output for same prompt\", same)\n",
    "write_to_outlog(\"Output for variant prompts\", variant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N completions VS N sentences in ONE completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N sentences in ONE completion\n",
    "def n_sentences():\n",
    "    outputs = []\n",
    "    base_prompt = \"Generate 5 different Maintenance Work Order (MWO) sentences describing the following equipment and undesirable event.\"\n",
    "    instruction = \"Each sentence can have a maximum of 8 words.\"\n",
    "    prompt = f\"{base_prompt}\\nEquipment: {path['object_name']}\\nUndesirable Event: {path['event_name']}\\n{instruction}\"\n",
    "    fewshot = get_5fewshot_message([base_prompt], [instruction])\n",
    "    message = fewshot + [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=message,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.9,\n",
    "                    n=1\n",
    "                )\n",
    "\n",
    "    for choice in response.choices:\n",
    "        output = choice.message.content\n",
    "        output = process_mwo_response(output)\n",
    "        for sentence in output:\n",
    "            outputs.append(sentence)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output in N completions:\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "Output within 1 completion:\n",
      "- park brake alarm fault detected\n",
      "- park brake shows alarm fault\n",
      "- alarm fault in park brake system\n",
      "- park brake is experiencing alarm fault\n",
      "- park brake alarm indicates fault\n"
     ]
    }
   ],
   "source": [
    "# Demo\n",
    "n_completion = same_prompt()\n",
    "n_sentence = n_sentences()\n",
    "\n",
    "print (\"Output in N completions:\")\n",
    "for c in n_completion:\n",
    "    print(f\"- {c}\")\n",
    "print (\"Output within 1 completion:\")\n",
    "for s in n_sentence:\n",
    "    print(f\"- {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique responses for N completions: 3\n",
      "Number of unique responses for ONE completion: 25\n"
     ]
    }
   ],
   "source": [
    "# Compare diversity over long run\n",
    "n_completion = []\n",
    "n_sentence = []\n",
    "for _ in range(20):\n",
    "    n_completion.extend(same_prompt())\n",
    "    n_sentence.extend(n_sentences())\n",
    "print (f\"Number of unique responses for N completions: {len(set(n_completion))}\")\n",
    "print (f\"Number of unique responses for ONE completion: {len(set(n_sentence))}\")\n",
    "write_to_outlog(\"Output in N completions\", n_completion)\n",
    "write_to_outlog(\"Output in 1 completion\", n_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrase VS Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paraphrase\n",
    "def paraphrase():\n",
    "    outputs = []\n",
    "    base_prompt = \"Generate a Maintenance Work Order (MWO) sentence describing the following equipment and undesirable event.\"\n",
    "    instruction = \"The sentence can have a maximum of 8 words.\"\n",
    "    prompt = f\"{base_prompt}\\nEquipment: {path['object_name']}\\nUndesirable Event: {path['event_name']}\\n{instruction}\"\n",
    "    fewshot = get_1fewshot_message([base_prompt], [instruction])\n",
    "    message = fewshot + [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=message,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.9,\n",
    "                    n=1\n",
    "            )\n",
    "\n",
    "    for choice in response.choices:\n",
    "        output = choice.message.content\n",
    "        output = process_single_response(output)\n",
    "        outputs.append(output)\n",
    "\n",
    "    keywords = [path['object_name'], path['event_name']]\n",
    "    paraphrases = paraphrase_mwo(client, output, keywords, 5)\n",
    "    outputs.extend(paraphrases)\n",
    "    outputs = list(set(outputs)) # Remove duplicates\n",
    "    return outputs[:5] # Only return 5 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for Paraphrasing:\n",
      "- alarm fault detected in the park brake system\n",
      "- the park brake shows an alarm fault detected\n",
      "- park brake alarm fault has been identified\n",
      "- park brake alarm fault detected\n",
      "- an alarm fault in the park brake was found\n",
      "Output for Generation:\n",
      "- park brake alarm fault detected\n",
      "- alarm fault in park brake system\n",
      "- park brake showing alarm fault\n",
      "- park brake has alarm fault\n",
      "- alarm fault occurring with park brake\n"
     ]
    }
   ],
   "source": [
    "# Demo\n",
    "paraphrases = paraphrase()\n",
    "n_sentence = n_sentences()\n",
    "\n",
    "print (\"Output for Paraphrasing:\")\n",
    "for p in paraphrases:\n",
    "    print(f\"- {p}\")\n",
    "print (\"Output for Generation:\")\n",
    "for s in n_sentence:\n",
    "    print(f\"- {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique responses for paraphrasing: 48\n",
      "Number of unique responses for generation: 24\n"
     ]
    }
   ],
   "source": [
    "# Compare diversity over long run\n",
    "paraphrases = []\n",
    "n_sentence = []\n",
    "for _ in range(20):\n",
    "    paraphrases.extend(paraphrase())\n",
    "    n_sentence.extend(n_sentences())\n",
    "print (f\"Number of unique responses for paraphrasing: {len(set(paraphrases))}\")\n",
    "print (f\"Number of unique responses for generation: {len(set(n_sentence))}\")\n",
    "write_to_outlog(\"Output for Paraphrasing\", paraphrases)\n",
    "write_to_outlog(\"Output for Generation\", n_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Paraphraser Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model for paraphrasing\n",
    "def model_paraphrase(tokenizer, model, sentences):\n",
    "    outputs = {}\n",
    "    for sentence in sentences:\n",
    "        batch = tokenizer([sentence],truncation=True,padding='longest',max_length=8, return_tensors=\"pt\")\n",
    "        translated = model.generate(**batch,max_length=15,num_beams=5, num_return_sequences=5, temperature=1.5)\n",
    "        output = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        outputs[sentence] = output\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\allis\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\allis\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\allis\\.cache\\huggingface\\hub\\models--t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\allis\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\allis\\.cache\\huggingface\\hub\\models--tuner007--pegasus_paraphrase. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\allis\\miniconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART Outputs:\n",
      "park brake alarm fault detected:\n",
      "- park brake alarm fault detected\n",
      "- park brake fault detected\n",
      "- park brake alarm fault fault detected\n",
      "- park brake alarm fault found\n",
      "- park brake alarm detected\n",
      "alarm fault in park brake system:\n",
      "- alarm fault in park brake\n",
      "- alarm fault in park brakes\n",
      "- alarm fault in park car\n",
      "- alarm fault in park assist\n",
      "- alarm fault in park system\n",
      "park brake has alarm fault:\n",
      "- park brake has alarm fault\n",
      "- park brake has warning light fault\n",
      "- park brake has alarm problem\n",
      "- park brake has alarm fault may cause\n",
      "- park brake has alarm fault may be\n",
      "park brake system shows alarm fault:\n",
      "- park brake system shows alarm fault\n",
      "- park brake system shows alarm fault in California\n",
      "- park brake system shows alarm fault in Florida\n",
      "- park brake system shows alarm fault in China\n",
      "- park brake system shows alarm fault in car\n",
      "alarm fault present in park brake:\n",
      "- alarm may be present in park\n",
      "- alarm fault present in South Korea\n",
      "- alarm fault present in South Africa\n",
      "- alarm fault present in US nuclear plant\n",
      "- alarm fault present in US\n",
      "The boy is walking happily on the street:\n",
      "- The boy is walking happily on\n",
      "- The boy is walking happily through the woods.\n",
      "- The boy is walking happily through the forest.\n",
      "- The boy is walking happily through the snow.\n",
      "- The boy is walking happily through the park.\n",
      "T5 Outputs:\n",
      "park brake alarm fault detected:\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected park brake alarm fault detected\n",
      "- park brake alarm fault detected park brake alarm fault detected park brake alarm fault\n",
      "- park brake alarm fault detected park brake alarm park brake alarm fault detected\n",
      "- park brake alarm fault detected park brake park brake alarm fault detected\n",
      "alarm fault in park brake system:\n",
      "- alarm fault in park brake system alarm fault in park brake system alarm fault\n",
      "- alarm fault in park brake system alarm fault in park brake system\n",
      "- park brake alarm fault in park brake system alarm fault in park brake system\n",
      "- park brake system alarm fault in park brake system alarm fault in park brake\n",
      "- alarm fault in park brake system.\n",
      "park brake has alarm fault:\n",
      "- parking brake has alarm fault\n",
      "- alarm fault alarm fault alarm fault alarm fault alarm fault alarm fault alarm\n",
      "- alarm fault alarm fault alarm fault alarm fault alarm fault alarm fault\n",
      "- parking brake has alarm fault parking brake has alarm fault\n",
      "- alarm fault alarm fault alarm fault alarm fault alarm fault\n",
      "park brake system shows alarm fault:\n",
      "- alarm fault alarm fault alarm fault alarm fault alarm fault alarm fault alarm\n",
      "- alarm fault alarm fault alarm fault alarm fault alarm fault alarm fault\n",
      "- alarm fault fault alarm fault alarm fault alarm fault alarm fault alarm fault\n",
      "- alarm fault alarm fault fault alarm fault alarm fault alarm fault alarm fault\n",
      "- alarm fault alarm fault alarm fault alarm fault alarm fault alarm fault\n",
      "alarm fault present in park brake:\n",
      "- alarm fault present in park brake alarm fault present in park brake alarm\n",
      "- park brake alarm fault present in park brake\n",
      "- park brake alarm fault present in park brake alarm fault present in park brake\n",
      "- alarm fault present in park brake alarm alarm fault present in park brake\n",
      "- alarm fault present in park brake alarm fault present in parking brake alarm\n",
      "The boy is walking happily on the street:\n",
      "- pavement.\n",
      "- carpet.\n",
      "- grass.\n",
      "- sidewalk.\n",
      "- paved path.\n",
      "Pegasus Outputs:\n",
      "park brake alarm fault detected:\n",
      "- The park brake alarm is malfunctioning.\n",
      "- The park brake alarm malfunctioned.\n",
      "- There is a park brake alarm.\n",
      "- The park brake alarm has a fault.\n",
      "- There is a park brake alarm fault.\n",
      "alarm fault in park brake system:\n",
      "- The park brake system has an alarm fault.\n",
      "- There is an alarm fault in the park brake system.\n",
      "- There was an alarm fault in the park brake system.\n",
      "- Park brake system has an alarm fault.\n",
      "- The park brake system had an alarm fault.\n",
      "park brake has alarm fault:\n",
      "- The park brake has an alarm.\n",
      "- There is an alarm fault in the park brake.\n",
      "- There is an alarm fault with the park brake.\n",
      "- The park brake has an alarm fault.\n",
      "- The park brake has a fault.\n",
      "park brake system shows alarm fault:\n",
      "- The park brake system has an alarm.\n",
      "- The park brake system has an alarm fault.\n",
      "- The park brake system has a fault.\n",
      "- The park brake system is malfunctioning.\n",
      "- The park brake system has an alarm malfunction.\n",
      "alarm fault present in park brake:\n",
      "- There is an alarm fault in the park brake.\n",
      "- The park brake has an alarm fault.\n",
      "- There is a fault in the park brake.\n",
      "- There was an alarm fault in the park brake.\n",
      "- There is a park brake alarm fault.\n",
      "The boy is walking happily on the street:\n",
      "- A boy is walking.\n",
      "- The boy is walking.\n",
      "- The boy is happy.\n",
      "- The boy is walking happily.\n",
      "- A boy is walking happily.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-large\")\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "\n",
    "sentences = [\"park brake alarm fault detected\",\n",
    "             \"alarm fault in park brake system\",\n",
    "             \"park brake has alarm fault\",\n",
    "             \"park brake system shows alarm fault\",\n",
    "             \"alarm fault present in park brake\",\n",
    "             \"The boy is walking happily on the street\"]\n",
    "\n",
    "bart_outputs = model_paraphrase(bart_tokenizer, bart_model, sentences)\n",
    "t5_outputs = model_paraphrase(t5_tokenizer, t5_model, sentences)\n",
    "pegasus_outputs = model_paraphrase(pegasus_tokenizer, pegasus_model, sentences)\n",
    "\n",
    "print(\"BART Outputs:\")\n",
    "for sentence, outputs in bart_outputs.items():\n",
    "    print(f\"{sentence}:\")\n",
    "    for output in outputs:\n",
    "        print(f\"- {output}\")\n",
    "print(\"T5 Outputs:\")\n",
    "for sentence, outputs in t5_outputs.items():\n",
    "    print(f\"{sentence}:\")\n",
    "    for output in outputs:\n",
    "        print(f\"- {output}\")\n",
    "print(\"Pegasus Outputs:\")\n",
    "for sentence, outputs in pegasus_outputs.items():\n",
    "    print(f\"{sentence}:\")\n",
    "    for output in outputs:\n",
    "        print(f\"- {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Generation Process\n",
    "For generating sentences for ONE path, we can use the following process:\n",
    "1. Use multiple **prompt variations**\n",
    "2. Generate **multiple sentences** for each completion using a random prompt\n",
    "3. Generate **paraphrases** for each generated sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 paths\n",
      "Completed 10 paths\n",
      "Completed 20 paths\n",
      "Completed 30 paths\n",
      "Completed 40 paths\n",
      "Completed 50 paths\n",
      "Completed 60 paths\n",
      "Completed 70 paths\n",
      "Completed 80 paths\n",
      "Completed 90 paths\n"
     ]
    }
   ],
   "source": [
    "base_prompts, instructions = initialise_prompts(client, num_variants=5, num_examples=5)\n",
    "all_sentences = []\n",
    "paths_50 = random.sample(data[408:], 100)\n",
    "sentences_50 = []\n",
    "\n",
    "# Generate sentences for paths\n",
    "for i, path in enumerate(paths_50):\n",
    "    # Variant prompt\n",
    "    if 'helper_name' in path:\n",
    "        prompt = get_generate_prompt(base_prompts, instructions, path['object_name'], path['event_name'], path['helper_name'])\n",
    "        keywords = [path['object_name'], path['event_name'], path['helper_name']]\n",
    "    else:\n",
    "        prompt = get_generate_prompt(base_prompts, instructions, path['object_name'], path['event_name'])\n",
    "        keywords = [path['object_name'], path['event_name']]\n",
    "\n",
    "    fewshot = get_5fewshot_message(base_prompts, instructions)\n",
    "    message = fewshot + [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Generation\n",
    "    sentences = []\n",
    "    response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=message,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.9,\n",
    "                    n=1\n",
    "                )\n",
    "    response_sentences = process_mwo_response(response.choices[0].message.content)\n",
    "    sentences.extend(response_sentences)\n",
    "    sentences = list(set(sentences)) # Remove duplicates\n",
    "\n",
    "    # Paraphrase\n",
    "    paraphrases = []\n",
    "    chosen_sentence = random.choice(sentences)\n",
    "    response_paraphrases = paraphrase_mwo(client, chosen_sentence, keywords, 5)\n",
    "    response_similarities = check_similarity(chosen_sentence, response_paraphrases)\n",
    "    for para, sim in zip(response_paraphrases, response_similarities):\n",
    "        if sim > 0.9:\n",
    "            paraphrases.append(para)\n",
    "    paraphrases = list(set(paraphrases)) # Remove duplicates\n",
    "\n",
    "    # Combine\n",
    "    sentences.extend(paraphrases)\n",
    "    sentences = list(set(sentences)) # Remove duplicates\n",
    "    # sentences_50.append(random.choice(sentences))\n",
    "    all_sentences.extend(random.sample(sentences, 2))\n",
    "    \n",
    "    # Print progress status\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Completed {i} paths\")\n",
    "    \n",
    "# Append to text file\n",
    "with open('TuringTest/synthetic_generate.txt', 'a', encoding='utf-8') as f:\n",
    "    for sentence in all_sentences:\n",
    "        f.write(f\"{sentence}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
