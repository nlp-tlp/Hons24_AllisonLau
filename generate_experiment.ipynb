{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\tpaths in object_property_paths\n",
      "567\tpaths in process_agent_paths\n",
      "402\tpaths in process_patient_paths\n",
      "1743\tpaths in state_patient_paths\n",
      "7\tpaths in object_property_state_paths\n",
      "0\tpaths in object_process_state_paths\n",
      "202\tpaths in state_agent_activity_paths\n",
      "44\tpaths in state_agent_patient_paths\n",
      "188\tpaths in process_agent_patient_paths\n",
      "Total number of paths: 3207\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from llm_generation import get_all_paths\n",
    "from llm_paraphrase import initialise_prompts, get_generate_prompt, paraphrase_mwo, process_mwo_response, check_similarity\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "client = OpenAI(api_key=api_key)\n",
    "data = get_all_paths(valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1fewshot_message(base_prompts, instructions):\n",
    "    \"\"\" Get fewshot message given fewshot csv file for single-example prompt \"\"\"\n",
    "    message = [{\"role\": \"system\", \"content\": \"You are a technician recording maintenance work orders.\"}]\n",
    "    with open(\"fewshot_messages/fewshot.csv\", encoding='utf-8') as f:\n",
    "        data = csv.reader(f)\n",
    "        next(data) # Ignore header\n",
    "        # Single-example prompt\n",
    "        for row in data:\n",
    "            if len(row) == 4:\n",
    "                if row[2] == \"\": # No helper\n",
    "                    user = {\"role\": \"user\", \"content\": get_generate_prompt(base_prompts, instructions, row[0], row[1])}\n",
    "                else:            # Has helper\n",
    "                    user = {\"role\": \"user\", \"content\": get_generate_prompt(base_prompts, instructions, row[0], row[1], row[2])}\n",
    "                assistant = {\"role\": \"assistant\", \"content\": row[3]}\n",
    "                message.append(user)\n",
    "                message.append(assistant)   \n",
    "    # Save fewshot message to json file\n",
    "    with open(\"fewshot_messages/fewshot1.json\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(message, f, indent=4)\n",
    "    return message\n",
    "\n",
    "def get_5fewshot_message(base_prompts, instructions):\n",
    "    \"\"\" Get fewshot message given fewshot csv file for 5-example prompt \"\"\"\n",
    "    message = [{\"role\": \"system\", \"content\": \"You are a technician recording maintenance work orders.\"}]\n",
    "    with open(\"fewshot_messages/fewshot.csv\", encoding='utf-8') as f:\n",
    "        data = csv.reader(f)\n",
    "        next(data) # Ignore header\n",
    "        # 5-example prompt\n",
    "        for row in data:\n",
    "            if len(row) > 4:\n",
    "                if row[2] == \"\": # No helper\n",
    "                    user = {\"role\": \"user\", \"content\": get_generate_prompt(base_prompts, instructions, row[0], row[1])}\n",
    "                else:\n",
    "                    user = {\"role\": \"user\", \"content\": get_generate_prompt(base_prompts, instructions, row[0], row[1], row[2])}\n",
    "                example = f\"1. {row[3]}\\n2. {row[4]}\\n3. {row[5]}\\n4. {row[6]}\\n5. {row[7]}\"\n",
    "                assistant = {\"role\": \"assistant\", \"content\": example}\n",
    "                message.append(user)\n",
    "                message.append(assistant)\n",
    "    # Save fewshot message to json file\n",
    "    with open(\"fewshot_messages/fewshot5.json\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(message, f, indent=4)\n",
    "    return message\n",
    "\n",
    "\n",
    "# Print some fewshot examples from MaintIE gold dataset\n",
    "def print_examples(object, event, helper=None):\n",
    "    \"\"\" Print some fewshot examples from the gold dataset \"\"\"\n",
    "    data = []\n",
    "    with open(\"data/MaintIE/gold_release.json\", encoding='utf-8') as f:\n",
    "        gold = json.load(f)\n",
    "        for d in gold:\n",
    "            text = d['text'].replace(\"<id> \", \"\").replace(\" <id>\", \"\")\n",
    "            data.append(text)\n",
    "    for sentence in data:\n",
    "        event_exists = re.search(rf'\\b{event}\\b', sentence)\n",
    "        helper_exists = re.search(rf'\\b{helper}\\b', sentence) if helper else None\n",
    "        if object in sentence and event_exists and helper_exists:\n",
    "            print(f\"{object},{event},{helper},{sentence}\")\n",
    "            return True\n",
    "        elif object in sentence and event_exists:\n",
    "            print(f\"{object},{event},{sentence}\")\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Process single response from the LLM\n",
    "def process_single_response(response):\n",
    "    \"\"\" Process single response from the LLM \"\"\"\n",
    "    response = response.lower()                     # Case folding\n",
    "    response = re.sub(r'[^\\w\\s]', ' ', response)    # Remove punctuation\n",
    "    response = re.sub(r\"\\s+\", \" \", response)        # Remove extra spaces\n",
    "    return response\n",
    "\n",
    "# write to outlog text file\n",
    "def write_to_outlog(title, data):\n",
    "    \"\"\" Write data to outlog text file \"\"\"\n",
    "    with open(\"outlog.txt\", \"a\", encoding='utf-8') as f:\n",
    "        f.write(\"========================================\\n\")\n",
    "        f.write(f\"{title}\\n\")\n",
    "        for d in data:\n",
    "            f.write(f\"{d}\\n\")\n",
    "        f.write(\"========================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air brake line,requires,replacing,air brake line blown - requires replacing\n",
      "bucket cylinder hose,blown,blown right hand bucket cylinder hose\n",
      "hydraulic hose,leaking,change out leaking hydraulic hose\n",
      "chain,fault,drag chain fault\n",
      "tank gauge,broken,aftercooler tank gauge broken\n"
     ]
    }
   ],
   "source": [
    "# Print some examples from the gold dataset\n",
    "successful_calls = 0\n",
    "while successful_calls < 5:\n",
    "    current = random.choice(data)\n",
    "    if 'helper_name' in current:\n",
    "        if print_examples(current['object_name'], current['event_name'], current['helper_name']):\n",
    "            successful_calls += 1\n",
    "    else:\n",
    "        if print_examples(current['object_name'], current['event_name']):\n",
    "            successful_calls += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same prompt VS Variant prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "path = random.choice(data)\n",
    "base_prompts, instructions = initialise_prompts(client, num_variants=5, num_examples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same prompt\n",
    "def same_prompt():\n",
    "    outputs = []\n",
    "    base_prompt = \"Generate a Maintenance Work Order (MWO) sentence describing the following equipment and undesirable event.\"\n",
    "    instruction = \"The sentence can have a maximum of 8 words.\"\n",
    "    prompt = f\"{base_prompt}\\nEquipment: {path['object_name']}\\nUndesirable Event: {path['event_name']}\\n{instruction}\"\n",
    "    fewshot = get_1fewshot_message([base_prompt], [instruction])\n",
    "    message = fewshot + [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=message,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.9,\n",
    "                    n=5\n",
    "                )\n",
    "\n",
    "    for choice in response.choices:\n",
    "        output = choice.message.content\n",
    "        output = process_single_response(output) \n",
    "        outputs.append(output)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant prompts\n",
    "def variant_prompt():\n",
    "    outputs = []\n",
    "    fewshot = get_1fewshot_message(base_prompts, instructions)\n",
    "    for _ in range(5):\n",
    "        prompt = get_generate_prompt(base_prompts, instructions, path['object_name'], path['event_name'])\n",
    "        message = fewshot + [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = client.chat.completions.create(\n",
    "                        model=\"gpt-4o-mini\",\n",
    "                        messages=message,\n",
    "                        temperature=0.9,\n",
    "                        top_p=0.9,\n",
    "                        n=1\n",
    "                )\n",
    "\n",
    "        for choice in response.choices:\n",
    "            output = choice.message.content\n",
    "            output = process_single_response(output)\n",
    "            outputs.append(output)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT given same prompt:\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "GPT given variant prompts:\n",
      "- park brake alarm fault detected\n",
      "- park brake has alarm fault\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault needs attention\n",
      "- park brake shows alarm fault\n"
     ]
    }
   ],
   "source": [
    "# Demo\n",
    "same = same_prompt()\n",
    "variant = variant_prompt()\n",
    "\n",
    "print (\"GPT given same prompt:\")\n",
    "for s in same:\n",
    "    print(f\"- {s}\")\n",
    "print (\"GPT given variant prompts:\")\n",
    "for v in variant:\n",
    "    print(f\"- {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique responses for same prompt: 3\n",
      "Number of unique responses for variant prompts: 5\n"
     ]
    }
   ],
   "source": [
    "# Compare diversity over long run\n",
    "same = []\n",
    "variant = []\n",
    "for _ in range(20):\n",
    "    same.extend(same_prompt())\n",
    "    variant.extend(variant_prompt())\n",
    "print (f\"Number of unique responses for same prompt: {len(set(same))}\")\n",
    "print (f\"Number of unique responses for variant prompts: {len(set(variant))}\")\n",
    "write_to_outlog(\"Output for same prompt\", same)\n",
    "write_to_outlog(\"Output for variant prompts\", variant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N completions VS N sentences in ONE completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N sentences in ONE completion\n",
    "def n_sentences():\n",
    "    outputs = []\n",
    "    base_prompt = \"Generate 5 different Maintenance Work Order (MWO) sentences describing the following equipment and undesirable event.\"\n",
    "    instruction = \"Each sentence can have a maximum of 8 words.\"\n",
    "    prompt = f\"{base_prompt}\\nEquipment: {path['object_name']}\\nUndesirable Event: {path['event_name']}\\n{instruction}\"\n",
    "    fewshot = get_5fewshot_message([base_prompt], [instruction])\n",
    "    message = fewshot + [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=message,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.9,\n",
    "                    n=1\n",
    "                )\n",
    "\n",
    "    for choice in response.choices:\n",
    "        output = choice.message.content\n",
    "        output = process_mwo_response(output)\n",
    "        for sentence in output:\n",
    "            outputs.append(sentence)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output in N completions:\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "- park brake alarm fault detected\n",
      "Output within 1 completion:\n",
      "- park brake alarm fault detected\n",
      "- park brake shows alarm fault\n",
      "- alarm fault in park brake system\n",
      "- park brake is experiencing alarm fault\n",
      "- park brake alarm indicates fault\n"
     ]
    }
   ],
   "source": [
    "# Demo\n",
    "n_completion = same_prompt()\n",
    "n_sentence = n_sentences()\n",
    "\n",
    "print (\"Output in N completions:\")\n",
    "for c in n_completion:\n",
    "    print(f\"- {c}\")\n",
    "print (\"Output within 1 completion:\")\n",
    "for s in n_sentence:\n",
    "    print(f\"- {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique responses for N completions: 3\n",
      "Number of unique responses for ONE completion: 25\n"
     ]
    }
   ],
   "source": [
    "# Compare diversity over long run\n",
    "n_completion = []\n",
    "n_sentence = []\n",
    "for _ in range(20):\n",
    "    n_completion.extend(same_prompt())\n",
    "    n_sentence.extend(n_sentences())\n",
    "print (f\"Number of unique responses for N completions: {len(set(n_completion))}\")\n",
    "print (f\"Number of unique responses for ONE completion: {len(set(n_sentence))}\")\n",
    "write_to_outlog(\"Output in N completions\", n_completion)\n",
    "write_to_outlog(\"Output in 1 completion\", n_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrase VS Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paraphrase\n",
    "def paraphrase():\n",
    "    outputs = []\n",
    "    base_prompt = \"Generate a Maintenance Work Order (MWO) sentence describing the following equipment and undesirable event.\"\n",
    "    instruction = \"The sentence can have a maximum of 8 words.\"\n",
    "    prompt = f\"{base_prompt}\\nEquipment: {path['object_name']}\\nUndesirable Event: {path['event_name']}\\n{instruction}\"\n",
    "    fewshot = get_1fewshot_message([base_prompt], [instruction])\n",
    "    message = fewshot + [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=message,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.9,\n",
    "                    n=1\n",
    "            )\n",
    "\n",
    "    for choice in response.choices:\n",
    "        output = choice.message.content\n",
    "        output = process_single_response(output)\n",
    "        outputs.append(output)\n",
    "\n",
    "    keywords = [path['object_name'], path['event_name']]\n",
    "    paraphrases = paraphrase_mwo(client, output, keywords, 5)\n",
    "    outputs.extend(paraphrases)\n",
    "    outputs = list(set(outputs)) # Remove duplicates\n",
    "    return outputs[:5] # Only return 5 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for Paraphrasing:\n",
      "- alarm fault detected in the park brake system\n",
      "- the park brake shows an alarm fault detected\n",
      "- park brake alarm fault has been identified\n",
      "- park brake alarm fault detected\n",
      "- an alarm fault in the park brake was found\n",
      "Output for Generation:\n",
      "- park brake alarm fault detected\n",
      "- alarm fault in park brake system\n",
      "- park brake showing alarm fault\n",
      "- park brake has alarm fault\n",
      "- alarm fault occurring with park brake\n"
     ]
    }
   ],
   "source": [
    "# Demo\n",
    "paraphrases = paraphrase()\n",
    "n_sentence = n_sentences()\n",
    "\n",
    "print (\"Output for Paraphrasing:\")\n",
    "for p in paraphrases:\n",
    "    print(f\"- {p}\")\n",
    "print (\"Output for Generation:\")\n",
    "for s in n_sentence:\n",
    "    print(f\"- {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique responses for paraphrasing: 48\n",
      "Number of unique responses for generation: 24\n"
     ]
    }
   ],
   "source": [
    "# Compare diversity over long run\n",
    "paraphrases = []\n",
    "n_sentence = []\n",
    "for _ in range(20):\n",
    "    paraphrases.extend(paraphrase())\n",
    "    n_sentence.extend(n_sentences())\n",
    "print (f\"Number of unique responses for paraphrasing: {len(set(paraphrases))}\")\n",
    "print (f\"Number of unique responses for generation: {len(set(n_sentence))}\")\n",
    "write_to_outlog(\"Output for Paraphrasing\", paraphrases)\n",
    "write_to_outlog(\"Output for Generation\", n_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Paraphraser Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def format_data():\n",
    "    finetune_data = []\n",
    "    with open('fewshot_messages/fewshot.csv', 'r', encoding='utf-8') as f:\n",
    "        data = csv.reader(f)\n",
    "        next(data) # Ignore header\n",
    "        for row in data:\n",
    "            if len(row) > 4:\n",
    "                original = row[3]\n",
    "                example = [row[4], row[5], row[6], row[7]]\n",
    "                for e in example:\n",
    "                    temp = {\n",
    "                        \"source\": original, \n",
    "                        \"target\": e\n",
    "                    }\n",
    "                    finetune_data.append(temp)\n",
    "                    \n",
    "    finetune_train = finetune_data[:int(0.8*len(finetune_data))]\n",
    "    finetune_val = finetune_data[int(0.8*len(finetune_data)):]\n",
    "    with open('fewshot_messages/train.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(finetune_train, f, indent=4)\n",
    "    with open('fewshot_messages/val.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(finetune_val, f, indent=4)\n",
    "\n",
    "# Transformer model for paraphrasing\n",
    "def model_paraphrase(model, tokenizer, sentence):\n",
    "    input_ids = tokenizer(\n",
    "        f'paraphrase: {sentence}',\n",
    "        return_tensors=\"pt\", padding=\"longest\",\n",
    "        max_length=25, truncation=True).input_ids.to(device)\n",
    "    outputs = model.generate(\n",
    "        input_ids, temperature=0.7, repetition_penalty=10.0,\n",
    "        num_return_sequences=5, no_repeat_ngram_size=2,\n",
    "        num_beams=5, num_beam_groups=5,\n",
    "        max_length=25, diversity_penalty=3.0\n",
    "    )\n",
    "    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return res\n",
    "\n",
    "def paraphrase_sentences(tokenizer, model, sentences):\n",
    "    outputs = {}\n",
    "    for sentence in sentences:\n",
    "        batch = tokenizer([sentence],truncation=True,padding='longest',max_length=25, return_tensors=\"pt\").to(device)\n",
    "        paraphrased = model.generate(**batch, temperature=0.7, repetition_penalty=10.0,\n",
    "                                        num_return_sequences=5, no_repeat_ngram_size=2,\n",
    "                                        num_beams=5, num_beam_groups=5,\n",
    "                                        max_length=25, diversity_penalty=3.0\n",
    "                                    )\n",
    "        output = tokenizer.batch_decode(paraphrased, skip_special_tokens=True)\n",
    "        outputs[sentence] = output\n",
    "    return outputs\n",
    "\n",
    "format_data()\n",
    "\n",
    "sentences = [\"park brake alarm fault detected\",\n",
    "             \"alarm fault in park brake system\",\n",
    "             \"park brake has alarm fault\",\n",
    "             \"park brake system shows alarm fault\",\n",
    "             \"alarm fault present in park brake\",\n",
    "             \"The boy is walking happily on the street\"]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART Paraphrase Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART Outputs:\n",
      "park brake alarm fault detected:\n",
      "- Park brake alarm fault detected\n",
      "- The alarm fault was detected in the park brake.\n",
      "- park Brake alarm fault detected.\n",
      "- Brake brake alarm Fault detected\n",
      "- Park brake alarm Fault detect\n",
      "alarm fault in park brake system:\n",
      "- Alarm Fault in park brake system?\n",
      "- What is the cause of an alarm fault in a park brake system?\n",
      "- The alarm fault is in park brake system.\n",
      "- In park brake system, alarm fault is not fixed.\n",
      "- System Fault in park brake system?\n",
      "park brake has alarm fault:\n",
      "- Park brake has alarm fault. How can it be fixed?\n",
      "- The park brake has alarm fault. How can it be fixed?\n",
      "- What is the cause of a brake alarm fault?\n",
      "- Park brakes have an alarm fault. What should I do to fix it?\n",
      "- park brake has alarm faults, it is not working.\n",
      "park brake system shows alarm fault:\n",
      "- Park brake system shows alarm fault.\n",
      "- The park brake system shows alarm fault.\n",
      "- Park brakes show alarm fault. How can this be fixed?\n",
      "- A parking brake system shows an alarm fault. How can this be fixed?\n",
      "- park brake system shows alarm faults, says the company.\n",
      "alarm fault present in park brake:\n",
      "- Alarm fault present in park brake?\n",
      "- In park brake, there is an alarm fault.\n",
      "- What are the symptoms of an alarm fault in a park brake?\n",
      "- There is an alarm fault in park brake.\n",
      "- The alarm fault in park brake is present.\n",
      "The boy is walking happily on the street:\n",
      "- The boy is happily walking on the street.\n",
      "- He is happily walking on the street.\n",
      "- A young boy is happily walking on the street.\n",
      "- Mr. Williams, who is in his early 20s, is happily walking on the street.\n",
      "- The boy is walking happily in the road, he said.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/eugenesiow/bart-paraphrase\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('eugenesiow/bart-paraphrase')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('eugenesiow/bart-paraphrase')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bart_model = bart_model.to(device)\n",
    "\n",
    "bart_outputs = paraphrase_sentences(bart_tokenizer, bart_model, sentences)\n",
    "\n",
    "print(\"BART Outputs:\")\n",
    "for sentence, outputs in bart_outputs.items():\n",
    "    print(f\"{sentence}:\")\n",
    "    for output in outputs:\n",
    "        print(f\"- {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Paraphrase Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "park brake alarm fault detected:\n",
      "- A park brake warning error has been found.\n",
      "- An accident has been reported with the park brake alert.\n",
      "- There has been a problem with the park brake alert system.\n",
      "- A parking brake alarm fault was detected.\n",
      "- The park brake alert system is running fine, but it has been detected.\n",
      "alarm fault in park brake system:\n",
      "- An alarm is raised in the park brake system.\n",
      "- An alarm system in the park brake system is failing.\n",
      "- There is an error in the park brake system that causes this alert.\n",
      "- an error occurred in the park brake system, causing an alarm.\n",
      "- In the park brake system, an alarm clock malfunctions.\n",
      "park brake has alarm fault:\n",
      "- The park brake has an alarm clock fault.\n",
      "- The park brake has an error, according to the driver.\n",
      "- A security issue with the park brake has been fixed.\n",
      "- An alarm is triggered by the park brake.\n",
      "- If the park brake has an error, it is possible to fix it.\n",
      "park brake system shows alarm fault:\n",
      "- The park brake system is showing an error.\n",
      "- An alarm clock is detected on the park brake system.\n",
      "- The park brake system sounds an alarm clock failure.\n",
      "- A warning light on the park brake system has been flashing.\n",
      "- An alarm system has failed in the park brake unit.\n",
      "alarm fault present in park brake:\n",
      "- In the park brake, an alarm clock malfunction is present.\n",
      "- An alarm clock fault is present in the park brake.\n",
      "- The park brake has an alarm system that is malfunctioning.\n",
      "- There is an alarm fault in the park brake system.\n",
      "- A warning light is triggered by the park brake, which is present in the vehicle.\n",
      "The boy is walking happily on the street:\n",
      "- The boy is walking happily on the street.\n",
      "- The child is enjoying his time on the street.\n",
      "- The little boy is walking happily on the pavement.\n",
      "- On the street, the boy is enjoying himself while walking happily.\n",
      "- He is walking happily on the street.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/ramsrigouthamg/t5_sentence_paraphraser\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "\n",
    "t5_sent_model = T5ForConditionalGeneration.from_pretrained(\"ramsrigouthamg/t5_sentence_paraphraser\")\n",
    "t5_sent_tokenizer = T5Tokenizer.from_pretrained(\"ramsrigouthamg/t5_sentence_paraphraser\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "t5_sent_model = t5_sent_model.to(device)\n",
    "\n",
    "for sentence in sentences:\n",
    "    outputs = model_paraphrase(t5_sent_model, t5_sent_tokenizer, sentence)\n",
    "    print(f\"{sentence}:\")\n",
    "    for output in outputs:\n",
    "        print(f\"- {output.removeprefix('paraphrasedoutput: ')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT Paraphrase T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "park brake alarm fault detected:\n",
      "- A fault in the park brake alarm has been identified.\n",
      "- Detection of a fault in the park brake alarm system\n",
      "- The park brake alarm has been deemed defective.\n",
      "- An error has been identified in the park brake alarm's functionality.\n",
      "- Park brake alarm faulty, please.\n",
      "alarm fault in park brake system:\n",
      "- An alarm failure has been detected in the park brake system.\n",
      "- The park brake system has been deemed to be defective due to an\n",
      "- A faulty alarm in the parking brake system has been detected.\n",
      "- There is an alarm failure in the park brake system.\n",
      "- Park brakes issue alarm\n",
      "park brake has alarm fault:\n",
      "- The park brake is experiencing an alarm failure.\n",
      "- Alarm jamming on park brake\n",
      "- Park brake with alarm faulty.\n",
      "- An alarm is malfunctioning on the park brake.\n",
      "- Alarm caliper failing on park brake.\n",
      "park brake system shows alarm fault:\n",
      "- The alarm is malfunctioning on the park brake system.\n",
      "- Alarm triggered by park brake system.\n",
      "- Alarm failure occurs in the parking brake system.\n",
      "- The park brake system is malfunctioning and the alarm has been activated\n",
      "- An alarm failure is causing the brakes to stop turning on the\n",
      "alarm fault present in park brake:\n",
      "- A fault in the park brakes is causing an alarm failure.\n",
      "- An alarm malfunction was observed in the park brakes.\n",
      "- The park brakes are experiencing an alarm failure.\n",
      "- Park brake issue with alarm.\n",
      "- There is an alarm failure on the parking brakes.\n",
      "The boy is walking happily on the street:\n",
      "- The boy is enjoying himself on the street.\n",
      "- With a smile on his face, the boy strolls along the\n",
      "- A happy boy walks along the street\n",
      "- Happy on the street: The boy walks along happily.\n",
      "- The boy walks in a cheerful manner on the street.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "chat_model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "chat_model = chat_model.to(device)\n",
    "\n",
    "for sentence in sentences:\n",
    "    outputs = model_paraphrase(chat_model, chat_tokenizer, sentence)\n",
    "    print(f\"{sentence}:\")\n",
    "    for output in outputs:\n",
    "        print(f\"- {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parrot Paraphrase Model T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "park brake alarm fault detected\n",
      "- ('park brake alarm fault detected', 0)\n",
      "alarm fault in park brake system\n",
      "- ('warning of a fault in the brake system', 17)\n",
      "park brake has alarm fault\n",
      "No paraphrases found.\n",
      "park brake system shows alarm fault\n",
      "- ('the park brakes system shows an alarm fault', 13)\n",
      "alarm fault present in park brake\n",
      "- ('warning of a fault in the park brake', 22)\n",
      "The boy is walking happily on the street\n",
      "- ('a happy boy walks along the streets', 26)\n",
      "- ('he walks happily on the street', 23)\n",
      "- ('the boy walks happily in the street', 19)\n",
      "- ('the boy walks happily on the street', 18)\n",
      "- ('the boy is walking happily on the street', 12)\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/prithivida/parrot_paraphraser_on_T5\n",
    "from parrot import Parrot\n",
    "\n",
    "parrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\", use_gpu=False)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    paraphrases = parrot.augment(input_phrase=sentence,\n",
    "                                diversity_ranker=\"levenshtein\",\n",
    "                                do_diverse=True, \n",
    "                                max_return_phrases=5, \n",
    "                                max_length=8)\n",
    "    if paraphrases:\n",
    "        for p in paraphrases:\n",
    "            print(f\"- {p}\")\n",
    "    else:\n",
    "        print(\"No paraphrases found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEGASUS fine-tuned for Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "park brake alarm fault detected:\n",
      "- The park brake alarm fault was detected.\n",
      "- Park brake alarm fault detected.\n",
      "- A fault with the park brake alarm has been detected.\n",
      "- There is a park brake alarm fault\n",
      "- Park brakes fault detected.\n",
      "alarm fault in park brake system:\n",
      "- There is an alarm fault in the park brake system.\n",
      "- The alarm fault in the park brake system.\n",
      "- Park brake system has an alarm fault.\n",
      "- It was a fault in the park brake system.\n",
      "- A fault in the park brake system was cited.\n",
      "park brake has alarm fault:\n",
      "- The park brake has an alarm fault.\n",
      "- Park brake has an alarm fault.\n",
      "- There is an alarm fault with the park brake.\n",
      "- The alarm fault is on the park brake.\n",
      "- A park brake has a problem\n",
      "park brake system shows alarm fault:\n",
      "- The park brake system has an alarm fault.\n",
      "- Park brake system shows alarm fault.\n",
      "- There is an alarm fault in the park brake system.\n",
      "- A park brakes alarm shows a fault.\n",
      "- The Park Brake System shows alarm fault\n",
      "alarm fault present in park brake:\n",
      "- There is an alarm fault in the park brake.\n",
      "- A fault in the park brake.\n",
      "- It was a fault in the park brake.\n",
      "- alarm fault present in the park brake\n",
      "- The alarm fault is present in the park brakes.\n",
      "The boy is walking happily on the street:\n",
      "- The boy is walking happily.\n",
      "- There is a boy walking on the street.\n",
      "- A boy walks happily on the street.\n",
      "- The child is on the street.\n",
      "- the kid walked happily on a road\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/tuner007/pegasus_paraphrase\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pegasus_model = pegasus_model.to(device)\n",
    "\n",
    "for sentence in sentences:\n",
    "    outputs = model_paraphrase(pegasus_model, pegasus_tokenizer, sentence)\n",
    "    print(f\"{sentence}:\")\n",
    "    for output in outputs:\n",
    "        print(f\"- {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Generation Process\n",
    "For generating sentences for ONE path, we can use the following process:\n",
    "1. Use multiple **prompt variations**\n",
    "2. Generate **multiple sentences** for each completion using a random prompt\n",
    "3. Generate **paraphrases** for each generated sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 paths\n",
      "Completed 10 paths\n",
      "Completed 20 paths\n",
      "Completed 30 paths\n",
      "Completed 40 paths\n",
      "Completed 50 paths\n",
      "Completed 60 paths\n",
      "Completed 70 paths\n",
      "Completed 80 paths\n",
      "Completed 90 paths\n"
     ]
    }
   ],
   "source": [
    "base_prompts, instructions = initialise_prompts(client, num_variants=5, num_examples=5)\n",
    "all_sentences = []\n",
    "paths_50 = random.sample(data[408:], 100)\n",
    "sentences_50 = []\n",
    "\n",
    "# Generate sentences for paths\n",
    "for i, path in enumerate(paths_50):\n",
    "    # Variant prompt\n",
    "    if 'helper_name' in path:\n",
    "        prompt = get_generate_prompt(base_prompts, instructions, path['object_name'], path['event_name'], path['helper_name'])\n",
    "        keywords = [path['object_name'], path['event_name'], path['helper_name']]\n",
    "    else:\n",
    "        prompt = get_generate_prompt(base_prompts, instructions, path['object_name'], path['event_name'])\n",
    "        keywords = [path['object_name'], path['event_name']]\n",
    "\n",
    "    fewshot = get_5fewshot_message(base_prompts, instructions)\n",
    "    message = fewshot + [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Generation\n",
    "    sentences = []\n",
    "    response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=message,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.9,\n",
    "                    n=1\n",
    "                )\n",
    "    response_sentences = process_mwo_response(response.choices[0].message.content)\n",
    "    sentences.extend(response_sentences)\n",
    "    sentences = list(set(sentences)) # Remove duplicates\n",
    "\n",
    "    # Paraphrase\n",
    "    paraphrases = []\n",
    "    chosen_sentence = random.choice(sentences)\n",
    "    response_paraphrases = paraphrase_mwo(client, chosen_sentence, keywords, 5)\n",
    "    response_similarities = check_similarity(chosen_sentence, response_paraphrases)\n",
    "    for para, sim in zip(response_paraphrases, response_similarities):\n",
    "        if sim > 0.9:\n",
    "            paraphrases.append(para)\n",
    "    paraphrases = list(set(paraphrases)) # Remove duplicates\n",
    "\n",
    "    # Combine\n",
    "    sentences.extend(paraphrases)\n",
    "    sentences = list(set(sentences)) # Remove duplicates\n",
    "    # sentences_50.append(random.choice(sentences))\n",
    "    all_sentences.extend(random.sample(sentences, 2))\n",
    "    \n",
    "    # Print progress status\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Completed {i} paths\")\n",
    "    \n",
    "# Append to text file\n",
    "with open('TuringTest/synthetic_generate.txt', 'a', encoding='utf-8') as f:\n",
    "    for sentence in all_sentences:\n",
    "        f.write(f\"{sentence}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
